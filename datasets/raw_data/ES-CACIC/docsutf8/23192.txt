Factores de matrices Cholesky parelelización y balance de carga
﻿

En este articulo se presentan las ideas rnas irnportantes para la paralelizacion de la factorizacion de matrices Cholesky. Se discuten dos aspectos basicos: la distribucion de los
calculos en distintos procesadores y la forrna en que la distribucion de estos calculos
sea similar en todos los procesadores. Para la distribucion de los calculos se tienen en
cuenta especiﬁcarnente las dependencias de datos y para la distribucion de la carga de
procesarniento se tienen en cuenta las caracteristicas propias de la secuencia de avarice
de procesarniento del rnétodo de factorizacion. Las ideas relacionadas con el balance de
carga son rnuy sirnilares a otros rnétodos de factorizacion y por lo tanto se pueden reusar
las forrnas de solucion que sean satisfactorias. Finalrnente, se rnuestran los resultados de
rendirniento obtenidos con distintas cantidades de procesadores en un cluster de PCs.

1. Introduccion

Los rnétodos nurnéricos han sido los prirneros candidatos a ser resueltos en las plataforrnas
de cornputo paralelo disponibles. De hecho, siernpre se han estudiado no solarnente las forrnas
de tener un control estricto de los errores nurnéricos sino tarnbién la forrna de optirnizar el
rendirniento. En el contexto de los usuarios de estos rnétodos para resolver sus propios problernas
de rnodelizacion, la biblioteca LAPACK (Linear Algebra PACKage) se ha establecido corno un
estandar de facto y corno rninirno en una referencia obligatoria [2] [7]  Teniendo en cuenta
la optirnizacion de rendirniento y especiﬁcarnente los tiernpos de respuesta de las operaciones,
se han desarrollado soluciones y optirnizaciones desde rniiltiples puntos de Vista:

I Las subrutinas de LAPACK se pueden clasiﬁcar de Varias rnaneras enfocandose especiﬁ—
carnente en la carga de procesarniento. Entre todas las subrutinas se han identiﬁcado
clararnente las que tienen rnayores requerirnientos de procesarniento y se las denorni—
na “rutinas cornputacionales”. Se han deﬁnido y estudiado clararnente corno las que se
tienen que optirnizar en cuanto a rendirniento y tiernpo de respuesta.

*InVestigador Asistente CICPBA

[9] J. J. Dongarra, J. Du Croz, 1. S. Duff, and S. Hammarling, “A set of Level 3 Basic Linear
Algebra Subprograms”, ACM Trans. Math. Soft., 16 (1990), pp. 1-17.

[10] Dongarra J., J. Du Croz, S. Hammarling, R. Hanson, “An extended Set of Fortran Basic
Linear Subroutines”, ACM Trans. Math. Soft., 14 (1), pp. 1-17, 1988.

[11] Dongarra J., D. Walker, “Libraries for Linear Algebra”, in Sabot G. W. (Ed), High Performance Computing: Problem Solving with Parallel and Vector Architectures, Addison-Wesley
Publishing Company, 1nc., pp. 93-134, 1995.

[12] Flynn M., “Very High Speed Computing Systems”, Proc. IEEE, Vol. 54, 1966.

[13] Flynn M., “Some Computer Organizations and Their Affectiveness”, IEEE Trans. on Computers, 21 (9), 1972.

[14] Golub C., C. Van Loan, Matrix Computations, 3rd Edition, The John Hopkins University
Press, ISBN 0-8018-5413-X, 1996.

[15] Gropp W., E. Lusk, N. Doss, A. Skjellum, “A high-performance, portable implementation
of the MP1 message passing interface standard”, Parallel Computing, Vol. 22, No. 6, pp.
789-828, Sep. 1996.

[16] Gustafson J. L., “Reevaluating Amdhal’s Law”, Communications of the ACM, 31 (5) pp.
532-533, 1988.

[17] Lawson C., R. Hanson, D. Kincaid, F. Krogh, “Basic Linear Algebra Subprograms for
Fortran Usage”, ACM Transactions on Mathematical Software 5, pp. 308-323, 1979.

[18] Snir M., S. Otto, S. Huss-Lederman, D. Walker, and J. Dongarra., MP1: The Complete
Reference, Volume 1 - The MP1-1 Core, 2nd edition. The MIT Press, 1998.

[19] Tinetti F. C., “Parallel Linear Algebra on Clusters”, VH Workshop de lnvestigadores en
Ciencias de la Computacion, WICC 2005, Universidad Nacional de Rio Cuarto, Facultad
de Ciencias EXactas, Fisico-Matematicas y Naturales, Departamento de Computacion, pp.

301-305, 13 y 14 de Mayo de 2005.

[20] Whaley R. C., A. Petitet, J. J. Dongarra, Automated Empirical Optimization of Software
and the ATLAS Project. Available at http://www.netlib.org/lapack/ lawns/lawn147.ps

[21] TOP500 Supercomputer Sites, http://www.top500.org

I Todas las rutirias de LAPACK (0 corno rniriirno las rutirias cornputacioriales) son implerneritadas en furiciéri de 0tr0 corijurito de rutirias deriorniriadas BLAS (Basic Linear
Algebra Subroutines) [10] [17] [11]. Estas rutirias son mas sericillas y por lo tarito tarnbiéri
mas sericillas en cuarito a la optirnizaciéri de reridirnierito. Optirnizarido las rutirias de

BLAS se optirnizari las rutirias de LAPACK.

I Las rutirias de BLAS se hari clasiﬁcado de acuerdo con los requerirnieritos de cérnputo en
tres riiveles: BLAS nivel 1, BLAS ﬂ1V€l 2 y BLAS riivel 3. Las rutirias de riivel 1 accederi
a O(n) datos (vectores) y realizari O(n) operaciories, las de riivel 2 accederi a O(n2) datos
(matrices) y realizari O(n2) operaciories y las de r1iVel3 accederi a O(n2) datos (matrices) y
realizari O(n3) operaciories. Por 10 tarito, la optirnizaciéri de reridirnierito debe ser aplicada
a las rutirias de riivel 3 

Es asi que se podriari corisiderar dos riiveles de cornplejidad en cuarito a la optirnizaciéri de las
rutirias sobre las cuales trabajar: las cornputacioriales de LAPACK y las de riivel 3 de BLAS. Es
claro que las cornputacioriales de LAPACK son mas cornplejas que las de nivel 3 de BLAS, pero
el aprovecharnierito de estas optirnizaciories tarnbiéri es mas directo. Se debe terier en cuerita
que las rutirias cornputacioriales de LAPACK son irnplerneritadas en furiciéri de las de BLAS,
pero su cornplejidad y requerirnieritos de procesarnierito no carnbiari.

Sin perder de Vista las aplicaciories riurnéricas, las arquitecturas de procesarnierito siernpre
hari sido terna de estudio en cuarito a las posibles forrnas de procesarnierito y sus correspondierites optirnizaciories [12] [13]. Las arquitecturas de procesarnierito paralelo son corisideradas
como la alterriativa mas fuerte en cuarito a la obtericiéri de tiernpo de respuesta razoriable de
las aplicaciories con rnayores requerirnieritos de cérnputo. De hecho, la evaluaciéri de la capacidad de cornputadoras para hacer la clasiﬁcaciéri TOP500 [6] [21] (donde se docurneritari las
500 rnaquirias mas rapidas existerites/evaluadas) se lleva a cabo con un prograrna paralelo.
Desde hace Varios aﬁos, las redes locales de cornputadoras de escrit0ri0 (tarnbiéri deriorniriadas
clusters) se corisiderari corno las plataforrnas mas coriveriierites en cuarito a la relaciéri cost0/berieﬁcio para cérnputo paralelo [3]  Los clusters tierieri Varias desveritajas en cuarito a
cérnputo paralelo, basicarnerite derivadas de que rii las redes locales rii las cornputadoras de
escrit0ri0 fuerori rii son diseﬁadas para este tipo de procesarnierito. Estas son buerias razories
por las cuales la evaluaciéri de reridirnierito paralelo que se preserita en este articulo se lleva a
cabo sobre uri cluster utilizado para cérnputo paralelo.

2. Factorizacién Cholesky

La factorizaciéri Cholesky se aplica a una rnatriz A 6 RM” sirnétrica deﬁriida positiva, y se
utiliza para ericoritrar uria rnatriz L E IRW” triangular inferior, tal que [14]:

A L LT
(L11 (L21 (L31 (L41 . . . [11 O . . . . . . . . . [11 [21 [31 [41
(L21 (L22 G32 G42 . . . [21 [22 O . . . . . . O [22 [32 [42
(L31 C132 (L33 (L43 . . . : [31 [32 [33 O . . . X . . . O [33 Z43 . . . 

G41 G42 (L43 (L44 . . . [41 [42 [43 Z44 . . . . . . . . . O [44

Por lo tanto, los elernentos de la rnatriz L se calculan con O(n3/3) operaciones que, especiﬁca—
rnente, son

j—1
lij Z (ClZ'j—Z:lZ'k><l]‘k)/ljj;  
[€21

2'—1
[€21

El patron de dependencias en los calculos es claro a partir de las ecuaciones anteriores. Segiin
la Ec. (2), para el calculo de [25 con  < i se utilizan los datos de la ﬁla i y la ﬁla  hasta la
colurnna i — 1 de arnbas ﬁlas. Segiin la Ec. (3), para el calculo de [M se utilizan los datos de la
rnisrna ﬁla i desde la prirnera colurnna hasta la colurnna i— 1. Puesto de otra rnanera, la prirnera
ﬁla no tiene ninguna dependencia de datos (se debe calcular solarnente el primer elernento y se
puede llevar a cabo inrnediatarnente a partir de la rnatriz a factorizar), y teniendo una ﬁla k
calculada se pueden calcular todos los elernentos de la colurnna is de las ﬁlas siguientes y luego
el elernento de la diagonal principal de la ﬁla k —|— 1.

3. Paralelizacion por Bloques de Filas

Con la idea anterior de dependencias, se puede llegar a una paralelizacion relativarnente sencilla, avanzando por ﬁlas de la rnatriz y considerando a los procesos interconectados en un
arreglo unidirnensional. Distribuyendo la rnatriz a factorizar por bloques de ﬁlas, todos los
procesadores tendran un conjunto de ﬁlas consecutivas de la rnatriz original (y a factorizar) y
tendran datos de las colurnnas de la rnatriz original (y a factorizar) de acuerdo con las ﬁlas que
tengan asignadas. La Fig. 1 rnuestra un ejernplo con una rnatriz de 8 X 8 elernentos distribuida

D a
an

EEE """""""""" '€”
DEED
iﬁiii ********* *2"
DDDDDD

 

DDDDDDDP.
DDDDDDDD

Figura 1: Distribucion en Bloques: Arreglo de Procesadores.

entre cuatro cornputadoras, P1, . . . , P4 (dos ﬁlas por procesador) interconectadas en un arreglo
unidirnensional de cornputadoras. Aunque la conectividad entre los procesadores puede ser
bidireccional, para este caso en particular los datos (ﬁlas de L) se cornunican en una sola
direccion, tal corno 10 indican las ﬂechas. Teniendo en cuenta esta distribucion, la Fig. 2 rnuestra
el pseudocodigo del procesarniento en cada cornputadora/procesador siguiendo el rnodelo de
prograrnacion por pasaje de rnensajes y el rnodelo de procesarniento SPMD (Single Prograrn,
Multiple Data). Se debe hacer notar que la cantidad de colurnnas con parte local depende del
procesador, y esta cantidad de colurnnas con parte local es la que deﬁne la iteracion principal
de procesarniento. En la Fig. 1 se puede Veriﬁcar clararnente que el primer procesador, por

/* El primer procesador comienza de manera especial */
if (primer procesador)
Computar el primer elemento de la diagonal

/* Columnas locales de las filas correspondientes */
for (k, columnas con parte local)
{
if (Fila k de un proceso anterior)
Recibir fila k del proceso anterior

if (Hay proceso siguiente)
Enviar fila k al proceso siguiente

Computar parte local de columna k (local) con la fila k

if (fila k+1 es local)
Computar el elemento (k+1, k+1)
}

Figura 2: Distribucion en Bloques: Computo y Comunicaciones.

ejemplo, tiene parte de solamente dos columnas y el ultimo tiene parte de todas las columnas.
Por otro lado, todo lo que se menciona como computar en el pseudocodigo corresponde a la
implementacion de la EC. (2) y la EC. (3) anteriores.

3.1. Breve Analisis de Rendimiento

En 1as bibliotecas de pasaje de mensajes 1as operaciones de transferencia de datos se llevan a
cabo en background (Como una operacion de entrada/salida estandar desde el punto de Vista del
sistema operativo). Esto imp1ica que el pseudocodigo anterior genera un procesamiento similar
a1 de un pipeline ya que, por ejemplo, mientras el primer procesador opera sobre la columna k,
e1 siguiente opera sobre 1a is — 1 y en general e1 procesador PC opera sobre la columna k — c —|— 1.
Sin embargo, en la paralelizacion por bloques de ﬁlas propuesta se pueden identiﬁcar problemas
de rendimiento casi inmediatamente. E1 mas evidente es el que surge de todas las factorizaciones
“right—1ooking” que transforman algun tipo de matriz densa en una o mas matrices triangulares,
tales como LU, QR y Cholesky [14]  En todas estas factorizaciones, 1as distribuciones por
bloques de ﬁlas consecutivas producen inconvenientes c1aros de rendimiento: a medida que se
avanza en el procesamiento, 1as primeras computadoras dejan de tener trabajo para realizar,
comienzan a estar cada Vez mas ociosas y el desbalance de carga es immediato. En el caso de
la factorizacion Cholesky, se agrega que, de hecho, 1as primeras computadoras tienen menor
cantidad de datos para procesar que las ultimas si la cantidad de ﬁlas por bloque es constante,
ta1 como la que se propone (Fig. 1).

4. Paralelizacion con Distribucion Ciclica de Filas

La forma mas usual de resolver e1 problema de desbalance de carga de las factorizaciones rightlooking es la distribucion ciclica de los datos. La idea basica es que todas las computadoras

tengan datos de la Inatriz a procesar en la mayorfa de las iteraciones. En el caso especiﬁco
de la factorizacién Cholesky esto se logra de manera sencilla si todas las cornputadoras tienen
ﬁlas que pertencen a distintos sectores de la Inatriz original, desde las prirneras ﬁlas hasta las
iiltirnas. Y esto es justarnente lo que se logra con la distribucién ciclica de las ﬁlas entre las
cornputadoras, que establece que la ﬁla i de la Inatriz estara asignada a la cornputadora PC si

czirnédcantc O§i§n—1 (4)

donde se asurne que las n ﬁlas se nurneran desde 0 hasta n — 1 y las numtasks cornputadoras
se nurneran desde 0 hasta cantc — 1. La Fig. 3 Inuestra esta distribucién para una rnatriz de

Fi1a0|:| Po
Fi1a1|:H:‘ P1
puazggg P2
5 IIII -P3
DEEDS
DDDDDD

; DDDDDDD
Fi1a7IIIIIIII

Figura 3: Distribucién Ciclica de las Filas de una Matriz.

8 X 8 elernentos a distribuir entre cuatro cornputadoras. La Fig. 4 Inuestra el pseudocédigo del
proceso que se ejecuta en la cornputadora PC en funcién de la distribucién ciclica de las ﬁlas

/* El primer procesador comienza de manera especial */
if (primer procesador)
Computar el primer elemento de la diagonal

/* Columnas locales de las filas correspondientes */
for (k, columnas con parte local)

{
if (Fila k no es local)
Recibir fila k del proceso c-1 mod numtasks

 

if (Fila k no es local de c+1)
Enviar fila k al proceso c+1 mod numtasks

Computar parte local de columna k (local) con la fila k

if (fila k+1 es local)
Computar el elemento (k+1, k+1)

Figura 4: Procesarniento en la cornputadora PC: Anillo de Procesadores.

de la Inatriz a factorizar. Siguiendo el ejernplo de la Fig. 3, y comparandolo con el algoritrno
de la seccién anterior, la cornputadora P0 tendra elernentos de las ﬁlas 0 y 4 y por lo tanto

tendra e1ernentos a procesar por mas de dos iteraciones. Dado que todas las cornputadoras
tendran e1ernentos de ﬁlas no consecutivas, 1a cornputadora P recibira ﬁlas de todas las dernas
C

orn utadoras no solarnente de las anteriores. Esto hace ue 1a interconexion entre las cornC p , q
putadoras en ani11o sea rnas apropiada que la de un arreg1o unidirnensional sirnp1e. Tarnbién
en este caso, e1 procesarniento de la factorizacion Cholesky tiene caracteristicas sirni1ares a la
rnayoria de los a1goritrnos para1e1os deﬁnidos en el area de algebra 1inea1: a) Prograrnacion con
pasaje de rnensajes, b) Procesarniento SPMD y c) Todas las cornunicaciones son punto a punto.
Por otro 1ado, todas las cornputadoras ahora tienen datos de casi todas las co1urnnas y por lo
tanto tendran datos a procesar en la rnayoria de las iteraciones 1o cua1, a su Vez, balancea 1a
carga de procesarniento.

5. Experimentacion

Se rea1iZaron experirnentos con los a1goritrnos para1e1os que se describen en la seccion anterior,
con el objetivo de eVa1uar su rendirniento. Los experirnentos se 11eVaron a cabo en un c1uster
de PCs. La Tabla 1 describe sintéticarnente 1as caracteristicas de las cornputadoras uti1iZadas.
La red de interconexion es Ethernet de 100 Mb/s y en el cab1eado se uti1iZa un unico switch de

CPU GHZ Mernoria Sisterna Operativo
Intel Pentium 4 2.4 1 GB Linux 2.4.18—14

Tabla 1: Rendirniento y Tiernpo Estirnado.

interconexion. E1 rendirniento se evalua en funcion de1 speedup obtenido, que es una rnétrica
rnuy conocida y util en el conteXto de los c1usters hornogéneos. Se uti1iZa 1a irnplernentacion
MPICH [15] de MP1 (Message Passing Interface) [18] para toda la cornunicacion de datos
entre procesos de la aplicacion para1e1a. La experirnentacion se 11eVo a cabo con matrices de
10000 X 10000 elernentos, que es suﬁcienternente grande corno para que el tiernpo de cornputo
sea signiﬁcativo y, por otro 1ado, e1 area de rnernoria necesaria es cercana a la disponible en
las cornputadoras. E1 tarnaﬁo no se esca1a con la cantidad de cornputadoras para Veriﬁcar e1
speedup de rnanera directa y para cornparar e1 speedup de arnbas forrnas de para1e1iZar basadas
en las distintas forrnas de distribuir los datos.

La Fig. 5 rnuestra 1os resu1tados de rendirniento de los a1goritrnos para1e1os que se describen anteriorrnente en el c1uster uti1iZando distintas cantidades de cornputadoras. La serie
de datos identiﬁcados con la etiqueta “B1oque”son 1os correspondientes a1 a1goritrno para1e1o
con distribucion de datos en bloques de ﬁlas. La serie de datos identiﬁcados con la etiqueta
“Cic1ico”son 1os correspondientes a1 a1goritrno para1e1o con distribucion cic1ica de las ﬁlas de la
rnatriz a factorizar. Finalrnente, la serie de datos identiﬁcados con la etiqueta “Optirno”son 1os
correspondientes a1 speedup optirno obtenible para cada cantidad de cornputadoras.

En principio, se puede notar a partir de los resu1tados obtenidos que el a1goritrno con
distribucion cic1ica de ﬁlas tiene rnejor rendirniento que el de distribucion de bloques de ﬁlas.
Adernas, a rnedida que la cantidad de cornputadoras aurnenta, 1a diferencia de rendirniento
tarnbién aurnenta en favor de1 a1goritrno con distribucion cic1ica.

La Tabla 2 rnuestra 1os Va1ores de speedup obtenido por los a1goritrnos para cada cantidad
de cornputadoras, donde se puede apreciar rnejor que, por ejernplo, para dos cornputadoras 1a
diferencia de speedup es de aproxirnadarnente 3.36 — 1.72 = 1.64 rnientras que para ocho, 1a

16

14

12

10

Speedup
(I)

 

2 4 8 16

Cantid ad (1 e Procesad ores

I Bloque 0 Ciclicc A Optimo

Figura 5: Experirnentacion con Matrices de 10000 X 10000 Elernentos.

Cantidad de Cornputadoras Speedup Bloque Speedup Ciclico
2 1.14 1.88
4 1.72 3.36
8 2.92 6.14
16 5.28 10.03

Tabla 2: Valores de Speedup para Matrices de 10000 X 10000 Elernentos.

diferencia de speedup es de aproxirnadarnente 6.14 — 2.92 : 3.23. Esto signiﬁca que para dos cornputadoras e1 algoritrno de distribucion cic1ica aprovecha 1a capacidad de procesarniento de 1.64
cornputadoras mas que el algoritrno de bloques y para ocho cornputadoras e1 aprovecharniento
es mayor: hace uso de la capacidad de 3.23 cornputadoras mas que el de bloque. La diferencia
de rendirniento en detrirnento de1 algoritrno de distribucion de bloques de ﬁlas esta basada en
el desbalance de carga producido por las diferencias de cantidad de datos a ca1cu1ar en cada
cornputadora. Con el objetivo de conﬁrrnar que el problerna de rendirniento de1 algoritrno de
distribucion de bloques es el desbalance de carga de procesarniento, se instrurnento e1 codigo
de forrna ta1 que se pueda identiﬁcar e1 tiernpo de procesarniento de cada cornputadora independienternente de las cornunicaciones. La Tabla 3 rnuestra 1os tiernpos relativos en ejecucion
de procesarniento para el algoritrno de distribucion de bloques. Se rnuestra e1 tiernpo de procesarniento de cada cornputadora con respecto a1 tiernpo total de procesarniento, que coincide
con el tiernpo de ﬁnalizacion de la cornputadora con el ultimo bloque de ﬁlas. Con estos Va1ores
se puede cuantiﬁcar rnejor e1 problerna de desbalance. La cornputadora que recibe el primer
bloque, por ejernplo, cornputa solarnente e1 0.2% de1 tiernpo total y el resto de1 tiernpo perrnanece ociosa, dado que no hay rnas datos a procesar en su bloque de ﬁlas. A rnedida que el
procesarniento avanza, rnas cornputadoras perrnanecen ociosas. Dado que, por ejernplo, 1a cornputadora con el bloque 9 solarnente cornputa durante un 36.9 % de1 total, e1 73.1 % de1 tiernpo
restante solarnente hay procesarniento en seis cornputadoras, 1as que reciben 1os bloques 10 a
15 de la rnatriz original. En el caso de1 algoritrno con distribucion cic1ica de ﬁlas, 1a diferencia
entre los tiernpos de ejecucion no supera e1 5 %.

Computadora (Bloque de Filas) Tiempo Relativo al Total (%)
0 0.2
1 1.0
2 2.8
3 5.3
4 8.7
5 12.9
6 17.4
7 23.1
8 29.6
9 36.9
10 45.0
11 53.9
12 63.6
13 74.1
14 85.3
15 100

Tabla 3: Tiempos Relativos de Procesamiento con Matrices de 10000 X 10000 Elementos.

Aunque los resultados de rendimiento del algoritmo con distribucion ciclica de ﬁlas sean
mejores que los de bloques de ﬁlas, esto no necesariamente signiﬁca que el rendimiento sea
optimo en si mismo. De hecho, en la Fig. 5 se puede Ver que la diferencia entre el speedup
obtenido por este algoritmo y el speedup optimo es cada Vez mayor. Quizas para este analisis
de rendimiento mas exhaustivo del algoritmo con distribucion ciclica conviene recurrir a la
métrica de eﬁciencia paralela. La Tabla 4 muestra los Valores de rendimiento en términos de

Cantidad de Computadoras Eﬁciencia
2 0.94
4 0.84
8 0.77
16 0.63

Tabla 4: Eﬁciencia del Algoritmo con Distribucion Ciclica, 10000 X 10000 Elementos.

eﬁciencia del algoritmo con distribucion ciclica de ﬁlas para matrices de 10000 X 10000 elementos
y distintas cantidades de computadoras. Si bien es cierto que al no escalar el tamaﬁo de las
matrices junto con la cantidad de computadoras se esta inmediatamente bajo la ley de Amdhal
[1] [16], también es posible que sean necesarias optimizaciones especiﬁcas orientadas a mejorar
la escalabilidad. Una de las formas de optimizacion mas directas a incluir es la distribucion por
bloques ciclica tal como la que se utiliza en [20] [2] para optimizacion de rendimiento secuencial
y en [5] para optimizacion de rendimiento paralelo.

6. Conclusiones y Trabajo Futuro

En este articulo se han presentado ideas de fundamental importancia sobre la paralelizacion
de la factorizacion de matrices Cholesky. También se comprobo la dependencia del rendimiento
con respecto a la distribucion de datos del algoritmo paralelo. El impacto de las factorizaciones
right—Iooking (entre las que se puede incluir la factorizacion Cholesky), sobre el balance de
carga en computo paralelo es fundamental. La experimentacion presentada en referencia a los
tiempos de computo relativos permite cuantiﬁcar la importancia del balance de carga relativa
al rendimiento paralelo.

El paso inmediato siguiente incluye la aplicacion de los principios de procesamiento con
distribucion ciclica de bloques a esta factorizacion. La idea es la combinacion de la distribucion
ciclica que resuelve el problema de balance de carga con el procesamiento por bloques que optimiza el rendimiento de toda la jerarquia de memoria. También se debe adaptar el algoritmo
paralelo, ahora incluyendo en la medida de lo posible las optimizaciones propias para la arqui—
tectura paralela de un cluster homogéneo tal como en [19]. Otra de las tareas aun por resolver
es la del balance de carga en los ambientes heterogéneos, que tiene su propia complejidad dada
por las diferencias de capacidad de computo relativas entre las computadoras disponibles.

