Aprendizaje de independencias específicas del contexto en Markov random fields
﻿en Markov Random Fields
Alejandro Edera y Facundo Bromberg
Facultad Regional Mendoza,
Universidad Tecnológica Nacional
{aedera,fbromberg}@frm.utn.edu.ar
Resumen Los modelos no dirigidos o Markov random fields son ampliamente utilizados para problemas que aprenden una distribución desconocida desde un conjunto de datos. Esto es porque permiten representar
una distribución eficientemente al hacer explícitas las independencias
condicionales que pueden existir entre sus variables. Además de estas independencias es posible representar otras, las Independencias Específicas
del Contexto (CSIs) que a diferencia de las anteriores sólo son válidas
bajo ciertos valores que pueden tomar subconjuntos de sus variables. Debido a esto son complicadas de representar y aprenderlas desde datos. En
este trabajo presentamos un enfoque para representar CSIs en modelos
no dirigidos y un algoritmo que las aprende desde datos utilizando tests
estadísticos. Mostramos resultados donde los modelos aprendidos por
nuestro algoritmo resultan ser mejores o comparables a modelos aprendidos por otros sin utilizar CSIs.
Keywords: Markov random fields, Context-Specific Independence, Ising
Model
1. Introducción
Los Modelos Probabilísticos Gráficos, o simplemente modelos gráficos,
permiten representar una distribución de probabilidad conjunta p(X) sobre el conjunto de variables aleatorias X, por medio de otra distribución
equivalente pΦ(X) factorizada en un conjunto de factores Φ [7], determinados por las independencias condicionales que existen en la distribución
p. La ventaja de esta factorización radica en el hecho de que cada factor
φ ∈ Φ es una función potencial definida sobre un subconjunto de variables, llamado el alcance del potencial, resultando en una reducción en la
complejidad espacial de almacenar una distribución en memoria. Muchos
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 41
problemas en la práctica consisten en aprender una distribución aproximada desde un conjunto de datos muestreados desde una distribución
desconocida. Dada las ventajas computacionales asociadas a utilizar modelos gráficos, estos problemas los utilizan durante el proceso de aprendizaje para representar la distribución. En la literatura pueden encontrarse
varios enfoques para realizar este aprendizaje desde datos, uno es el enfoque Independence-Based. Los algoritmos que implementan este enfoque
aprenden un modelo primero aprendiendo sus independencias ejecutando
una sucesión de tests estadísticos y en base a estas definen un conjunto
de potenciales cuyos parámetros son aprendidos utilizando algún procedimiento de optimización numérica. En este trabajo nos focalizamos en el
problema de aprender distribuciones desde datos utilizando modelos no
dirigidos con Independencias Específicas del Contexto – Context-Specific
Independencies (CSIs) [3,9]. Estas, a diferencia de las independencias no
contextualizadas, tienen la particularidad de ser válidas dada una determinada asignación a un subconjunto de variables de su condicionante,
denominado contexto de la independencia. Estas son bastantes intuitivas
en problemas que requieren modelos dirigidos debido a la naturaleza de
sus dependencias, por esto su gran uso en estos escenarios [4,5,8,10]. Sin
embargo, aún no han sido ampliamente adoptadas en escenarios donde se
necesitan modelos no dirigidos, por la dificultad de representar las independencias contextuales debido a su naturaleza espacial. De este modo,
presentaremos un enfoque para representar independencias contextuales
que hace uso de una representación más expresiva de un modelo no dirigido. Luego presentaremos un algoritmo que aprende independencias
contextuales con el enfoque Independence-Based. Para evaluar los modelos aprendidos por nuestro algoritmo, tuvimos en cuenta datos generados
desde distribuciones representadas por Ising models. Nuestros resultados
muestran que los modelos con independencias contextuales resultan ser
mejores o comparables con respecto a los modelos sin ellas.
2. Modelos gráficos no dirigidos
Un modelo gráfico para una distribución p(X) consiste de un grafo
G y un conjunto de parámetros numéricos θ, donde G = (V,E) es un
conjunto de n = |V | nodos enlazados por aristas (s, t) ∈ E y cada vaCACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 42
riable aleatoria Xs ∈ X se asocia con un nodo s ∈ V . Denotamos por
x a una asignación a las variables X. Un clique es un subgrafo de G
donde todos sus nodos están completamente conectados por aristas. El
grafo G se denomina la estructura de independencias o estructura del
modelo y codifica eficientemente un conjunto de independencias condicionales entre las variables X. Denotaremos por XA ⊆ X al conjunto
de variables aleatorias asociadas con un conjunto de nodos A ⊆ V y
su dominio con Val(XA) = ⊗s∈AVal(Xs) el producto cartesiano de los
dominios Val(Xs) = {x1s, x
2
s, . . . , x
k
s}. Decimos que las variables XA son
independientes de XB dado XC , denotado por (XA ⊥ XB | XC), si y
sólo si en G no existe ningún camino desde los nodos A a los nodos B al
remover los nodos C. Una distribución donde sus independencias pueden
ser representadas de esta manera, se denominan graph-isomorph. En este
trabajo asumimos este tipo de distribuciones.
Para facilitar nuestra posterior presentación utilizaremos una representación exponencial de un modelo no dirigido. Para algún conjunto de
indices I, sea Φ = {φα, α ∈ I} un conjunto de funciones potencial cuyos
alcances se corresponden con cliques de G, entonces definimos un modelo
no dirigido como:
pΦ(X) = exp{
∑
α∈I
θαφα(X)− Z}, (1)
donde la constante Z sirve para obtener una distribución normalizada.
Cada potencial φα es un mapeo de la forma: Xφα 7→ R+. En en este trabajo sólo tendremos en cuenta potenciales que son funciones indicador, las
cuales toman el valor 1 si la asignación x es igual a la α-ésima asignación
de las variables de su alcance, denotado por Scope[φα], en cualquier otro
caso toma el valor 0. Dadas estas funciones indicador, entonces definimos
el conjunto I para indexar el conjunto de todas las asignaciones posibles
para los cliques C, es decir, I = {1, . . . , |ξ|}, donde ξ es la unión de todas
las asignaciones de la forma ξ =
⋃|C|
i Val(XCi).
Finalmente, es interesante analizar (1) y remarcar que un modelo no
dirigido está determinado por su conjunto Φ de funciones indicador. Modificar alguna de ellas, por ejemplo removiendo variables de sus alcances,
cambiaría el valor que esta retorna, resultando en un modelo totalmente
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 43
distinto. Nuestro enfoque para representar independencias contextuales
en modelos no dirigidos consiste en modificar las asignaciones ξ, con lo
cual se modifican los alcances de sus funciones indicador asociadas. A continuación daremos detalle de cuáles funciones indicador modificaremos y
cómo las modificaremos.
3. Contextualización de un modelo no dirigido
Comencemos extendiendo la definición de independencia condicional
agregando contextos. Dado un conjunto de variables XA y XB decimos
que son contextualmente independientes dado XC y un contexto xU , denotado por (XA ⊥ XB | XC , xU ), si en G no existe ningún camino desde
los nodos A a los nodos B cuando eliminamos los nodos C∪U . Factorizando la distribución p(XA, XB | XC , xU ) a p(XA | XC , xU ) p(XB | XC , xU )
[3]. Nos referiremos como modelo contextualizado a un modelo con estas
independencias.
Decimos que una asignación xC es compatible con otra asignación
x′U si xC = x′U∩C . De igual manera, sea ξ un conjunto de asignaciones
denotamos por ξ[x′U ] al subconjunto de asignaciones compatibles con la
asignación x′U . Definimos al conjunto C de una asignación xC como su
alcance y lo denotamos por Scope[xC ].
Entonces dada las asignaciones ξ de los potenciales de un modelo
representamos una independencias contextuales según la siguiente propiedad:
Proposición 1 Dado los conjuntos A,B,C,U y sea ξ un conjunto de
asignaciones compatibles con xU , si se cumple la independencia contextual (XA ⊥ XB | XScope[ξ]\{A,B,U}, xU ), entonces las asignaciones ξ las
podemos representar como los conjuntos de asignaciones ξ′ y ξ′′, donde
toda asignación x′ ∈ ξ′ cumple Scope[x′]∩B = ∅ y toda asignación x′′ ∈ ξ′′
cumple Scope[x′′] ∩A = ∅.
Demostración. Dado que una distribución condicionada es proporcional
a su conjunta y suponiendo que (XA ⊥ XB | XScope[ξ]\{A,B,U}, xU ) entonces por [6] obtenemos que p(XA, XB | XScope[ξ]\{A,B,U}, xU ) factoriza en el producto de factores p(XA | XScope[ξ]\{A,B,U}, xU ) y p(XB |
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 44
XScope[ξ]\{A,B,U}, xU ). Estos factores pueden ser representados utilizando
la representación exponencial (1), con lo cual obtenemos ξ′ y ξ′′.
uunionsq
Entonces teniendo (XA ⊥ XB | XC , x′U ) aplicamos la prop. 1 sobre las
asignaciones ξ de algún clique de C utilizando la operación Split(ξ, A,B).
Esta operación factoriza ξ de acuerdo a la independencia retornando el
conjunto ξ′ ∪ ξ′′, donde ξ′ = {xScope[x]\A | x ∈ ξ} y ξ
′′ = {xScope[x]\B | x ∈
ξ}.
4. Context Separation Algorithm
En el alg. 1 mostramos nuestro algoritmo denominado Context Separation (CS), el cual trabaja sobre las asignaciones ξ de las funciones
indicador de un modelo representado como (1). El núcleo del algoritmo
consiste en proponer una serie de independencias contextuales de la forma (Xs ⊥ Xt | xU ) que son verificadas ejecutando tests estadísticos sobre
los datos, que en caso de satisfacerse, son representadas sobre ξ siguiendo
la prop. 1. Proponer las independencias implica determinar qué variables
XU del modelo se contextualizarán. La forma más sencilla para determinarlas sería por cada par de variables Xs y Xt seleccionar como XU
a las n − 2 variables restantes y luego por cada conjunto W de los 2|U |
conjuntos para U definir sus |Val(XW )| contextos. Claramente esta forma
se torna intratable a medida que crece n. Nuestro algoritmo propone una
aproximación que consiste en tomar cada clique de G y para cada par Xs
y Xt de variables, contextualizar solamente con las restantes variables del
clique. Una vez finalizadas todas estas verificaciones, el algoritmo retorna
un conjunto de asignaciones que representan la estructura contextualizada
de G.
Hablando a grandes rasgos, CS está formado por dos bucles anidados.
El bucle externo, representado por la función CS, comienza iterando en
la línea 3 por un conjunto de cliques C bajo algún ordenamiento dado.
Por cada clique Ci, la función build-assigments, lo representa como el
conjunto de asignaciones Val(XCi) de las variables XCi siguiendo (1). En
la línea 5 se verifica si el número de variables del clique Ci es menor a dos,
en tal caso el algoritmo no continua y toma el siguiente clique en C. Esto
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 45
Algorithm 1: Context Separation Algorithm.
Procedure CS(C)1
ξ ← ∅2
foreach Ci ∈ C do3
ξ ← ξ ∪ build-assignments(Ci)4
if |Ci| < 2 then continue5
foreach s, t ∈ Ci do6
foreach W in the power set of Ci \ {s, t} do7
ξ ← ξ ∪ separation(ξ, s, t, W , ∅, ∅)8
return ξ9
10
Procedure separation(ξ, s, t, W , C, x)11
if W \ C = ∅ then return ξ12
C′ ←W \ C13
u← first(C′)14
foreach xu ∈ Val(Xu) do15
ξ′ ← ξ[xu]16
ξ′′ ← ξ \ ξ′17
if (Xs ⊥ Xt | XC′\u, x ∪ xu) then ξ
′ ← Split(ξ′, s, t)18
ξ ← ξ′′ ∪ separation(ξ′, s, t, W , C ∪ u, x ∪ xu)19
return ξ20
21
es así porque para determinar independencias se necesita al menos dos variables. Luego, en las líneas 6 y 7, se seleccionan cuáles de las variables del
clique XCi serán utilizadas para definir los contextos de las independencias que se ejecutarán durante el bucle interno. Esta selección es nuestra
aproximación para obtener los contextos de las independencias y consiste
en: para cada s, t ∈ Ci se obtiene el conjunto W ⊂ Ci, donde W pertenece al power set de Ci \ (s, t). El costo de nuestra aproximación está en
función del clique con mayor número de variables en G, denotemos a este
número por k∗ ≤ n, entonces la aproximación tiene un costo de O(2k
∗−2)
frente a los O(2n−2) de no realizarla. Una justificación de esta aproximación se debe a las Markov Properties [7], hablando aproximadamente
las dependencias que tiene una variable son más fuertes entre aquellas
variables cuyos nodos son vecinos en la estructura G. Dado que un contexto implica una cierta dependencia entre las variables involucradas en
la independencia contextual, entonces se puede suponer que la mayoría
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 46
de las CSIs ocurran entre variables cuyos nodos son próximos en G. La
efectividad de esta suposición es corroborada por nuestros experimentos.
El bucle interno, realizado por la función separation, inicialmente
recibe las asignaciones ξ del clique Ci, los indices s, t ∈ Ci y el conjunto
W . En base a estas evaluará, en línea 18, si las variables Xs y Xt son contextualmente independientes dado una asignación parcial a las variables
XW . Si la independencia se satisface en los datos, entonces se toma el
subconjunto de ξ compatible con el contexto de la independencia y, como
detalla la línea 18, se le aplica la operación de Split. Las asignaciones
parciales para las variables XW se construyen recursivamente en base al
conjunto C y la asignación x, que llevan la pista de las variables y valores
que han sido asignados en la recursión anterior. Por esto, inicialmente
C y x son vacías. Por lo tanto, una asignación recursiva procede de la
siguiente manera: en la línea 14 se toma la variable Xu donde u ∈W \C
y se itera en la línea 15 por sus asignaciones xu ∈ Val(Xu) haciendo una
llamada recursiva en línea 19 con la asignación parcial x ∪ xu y el nuevo
conjunto C ∪ u. Esto se detiene, en la línea 12, una vez que ya han sido
asignados todos los valores posibles para las variables XW .
5. Experimentación
Nuestros experimentos consisten en aprender modelos desde conjuntos de datos generados desde distribuciones subyacentes conocidas, Ising
models binarios con la siguiente forma: p(X) = exp{
∑
s∈V θsφs(Xs) +∑
(s,t)∈E θstφst(Xs∪t) − Z}. Donde φs(Xs) = 1 si xs = 1 y φst(Xs∪t) = 1
si xs = 1 y xt = 1. Los parámetros fueron elegidos aleatoriamente
para generar distintas instancias del modelo. En todos los casos elegimos el parámetro del nodo s desde θs ∼ N (0, σ2node). Para cada arista
(s, t), establecimos su parámetro, independientemente de las restantes,
como θst ∼ |N (0, σ2edge)|. Para todos los experimentos tuvimos en cuenta σnode = 0.5 σedge, y fuimos variando la elección de σedge ∈ {0.1, 0.3,
0.6, 0.9}. Mientras mayor es σedge menor es la fuerza de las dependencias
en el modelo y su aprendizaje se torna más duro. Generamos diez Ising
models diferentes por cada valor de σedge que muestreamos utilizando un
Gibbs Sampler que descartó las primeras 10000 muestras (burn-in phase)
y luego tomó 1000 muestras para armar un D.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 47
Elegimos como competidor de CS al algoritmo HHC, uno de los algoritmos del estado del arte para aprender estructuras no contextualizadas
desde conjuntos de datos [1]. Este algoritmo, al igual que CS, utiliza tests
estadísticos durante el aprendizaje, de esta manera para ser justa la evaluación en ambos algoritmos utilizamos el test Pearson X 2. Debido a que
HHC fue diseñado para aprender modelos dirigidos, tuvimos que adaptarlo para nuestros experimentos. La adaptación consistió solamente en
omitir su paso final en cual orienta las aristas de la estructura aprendida.
Como explicamos en la sec. 3, el algoritmo CS necesita como entrada un
conjunto de cliques C que representan a una estructura G. En nuestros
experimentos utilizamos la estructura aprendida por HHC, obteniendo
sus cliques utilizando el algoritmo Bron–Kerbosch.
Una vez obtenidas las estructura aprendidas por CS y HHC es necesario aprender sus parámetros para obtener un modelo, esto lo realizamos
maximizando la función log-likelihood:
LL(θ) =
1
M
M∑
m
log p(D[m]) =
∑
α∈I
θαPˆ − Z,
donde Pˆ := 1M
∑M
m φα(D[m]) es la suma de los valores del potencial
sobre los datos, es decir, su promedio empírico en los datos. Tomando las
derivadas de LL e igualándolas a cero encontramos que la solución θML
es moment-matching, es decir, satisface EθML [φα] = Pˆ . Esto significa que
los momentos de la distribución del modelo con parámetros θML están
emparejados con los promedios empíricos Pˆ de sus funciones indicador.
Para esta optimización utilizamos L-BFGS.
Utilizamos como métrica para evaluar la calidad de los modelos aprendidos la KL divergence que tiene la forma:
KL(p || q) =
∑
x∈Val(X)
p(x) log
p(x)
q(x)
,
donde es igual a cero cuando p = q y positiva en cualquier otro caso.
Esta puede ser interpretada como una “distancia” de una distribución p
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 48
(a) σedge = 0.1 (b) σedge = 0.3
(c) σedge = 0.6 (d) σedge = 0.9
Figura 1. KL divergence para los modelos aprendidos desde conjuntos de datos por los
algoritmos CS y HHC a medida que la cantidad de datos crece. Cada subfigura indica
la dificultades del aprendizaje, mientras mayor σedge más duro es.
con respecto a otra distribución q en términos de su cantidad de información.
En la figura 5 mostramos los valores de KL promediados para 10 corridas sobre un número de datos creciente generados de grillas 4×4. Cada
una de las subfiguras corresponde a diferentes escenarios de aprendizaje
representados por los diferentes σedge que elegimos, siendo la esquina inferior derecha el escenario más duro. En todos los resultados podemos ver
que para un reducido número de datos los modelos contextualizados resultan ser superiores a los no contextualizados y mientras los datos crecen
ambos se tornan comparables. Los valores comparables de KL no ocurren
por que se degrade la calidad de los modelos contextualizados, sino que se
obtienen mejores modelos no contextualizados. La causa de estas mejoras
se debe a la propiedad moment-matching del aprendizaje de parámetros.
El aprendizaje de parámetros asigna parámetros cercanos a cero a aquellas funciones indicadores que tienen una baja frecuencia en los datos, al
no tenerse en cuenta estas funciones en los modelos se obtiene un comportamiento similar al realizado por el algoritmo CS. Esta explicación es
respaldada por el hecho de que los peores modelos no contextualizados
son aprendidos para σedge grandes, es decir donde las independencias de
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 49
la distribución subyacente son débiles, necesitando el aprendizaje de parámetro una mayor cantidad de datos para hacer parámetros cercanos a
cero.
6. Trabajo futuro
Hay muchas maneras en las cuales nuestro trabajo puede ser extendido. La principal es que nuestro algoritmo necesita de otro para obtener el
conjunto inicial de cliques de una estructura, en base a los cuales aprende las independencias contextuales. En nuestros experimentos los cliques
son suministrados por una versión modificada del algoritmo HHC. Uno
de nuestros trabajos a futuro consistirá en extender las ideas expuestas
de CS y diseñar un nuevo algoritmo que para un conjunto dado de datos
aprenda las independencias contextualizadas para un modelo sin necesidad de suministrarle un conjunto inicial de cliques.
