Un método para la fragmentación vertical de bases de datos y su variante como evaluador de particiones
﻿ El diseño de bases de datos distribuidas es un problema de 
optimización que implica la solución de problemáticas como la fragmentación 
de los datos y su ubicación. Típicamente, los criterios que determinan si la 
fragmentación y la asignación son óptimas se establecen de manera 
independiente. Primero se busca la “mejor” fragmentación y luego la “mejor” 
ubicación de los fragmentos obtenidos. La fragmentación vertical es más 
complicada que la partición horizontal, debido al incremento del número de 
posibles alternativas. En este trabajo se presenta un nuevo método para la 
fragmentación vertical, que se basa fundamentalmente en la Matriz de 
Atracción entre Atributos, suplantando la conocida Matriz de Afinidad entre 
Atributos. Se utiliza como heurística el enfoque de agrupamientos jerárquicos y 
una regla de decisión basada en la homogeneidad interna y la heterogeneidad 
externa de los grupos obtenidos. También se presenta una variante para que 
pueda ser usado como evaluador de particiones. 
Palabras claves: Bases de datos distribuidas, evaluador de particiones, 
fragmentación vertical, medida de afinidad, método jerárquico, regla de 
decisión. 
1   Introducción 
El interés por el desarrollo de bases de datos distribuidas relacionales ha aumentado 
en la medida que las organizaciones y empresas han crecido y se han expandido 
geográficamente; de la mano del vertiginoso progreso en el uso de redes de 
computadores. El diseño de bases de datos distribuidas es un problema de 
optimización que implica la solución de problemáticas como la fragmentación de los 
datos, su ubicación y replicación. 
La fragmentación es el proceso mediante el cual una relación global es 
descompuesta en fragmentos horizontales y/o verticales. Un fragmento vertical 
atiende al agrupamiento de datos en función de atributos o conjuntos de ellos, 
mientras que la fragmentación horizontal atiende a dicho agrupamiento en función de 
                                                          
 
 
tuplas o conjuntos de tuplas. Típicamente, los criterios que determinan si la 
fragmentación y la asignación son óptimas se establecen de manera independiente, en 
dos pasos. En el primero se busca la “mejor” fragmentación y, en el segundo, se busca 
la “mejor” ubicación de los fragmentos obtenidos en el paso anterior [1]. 
Comparando las formas de fragmentación, la partición vertical es más complicada 
que la partición horizontal, debido al incremento del número de posibles alternativas 
[2]. Como se señala en [3], un objeto con m atributos puede ser particionado de B(m) 
diferentes formas, donde B(m) es el m-ésimo número de Bell, para m suficientemente 
grandes, B(m) se aproxima a mm; para m=15 este es ≈ 109, para m=30 este es ≈ 1023. 
Por lo tanto, es importante contar con una estrategia que reduzca de manera 
eficiente el número de cálculos. Aunque diferentes, dos enfoques secuenciales que 
conducen a agrupamientos jerárquicos parecen haber adquirido un interés particular 
entre los taxónomos. Una de las estrategias es el algoritmo propuesto por Ward 
(1963). Su idea es aglomerar los puntos o los grupos resultantes, reduciendo su 
número en uno en cada etapa  de un procedimiento de fusión secuencial, hasta que 
todos los puntos estén en un único clúster. Un algoritmo contrario ha sido propuesto 
por Edwards y Cavalli-Sforza (1965). La esencia de su método es la partición 
consecutiva de un conjunto de puntos en dos subconjuntos: primero un conjunto 
inicial es dividido en dos grupos, cada uno de ellos se subdivide en dos grupos más 
pequeños por separado, y así sucesivamente, hasta que se alcancen los puntos 
individuales [4]. 
En la presente investigación se utiliza el enfoque propuesto por Ward, aunque el 
método propuesto se puede implementar independientemente con cualquiera de los 
dos enfoques. 
En la mayoría de las situaciones de agrupación de la vida real, un investigador 
aplicado se enfrenta con el dilema de seleccionar el número de grupos o particiones 
en la solución final. Los procedimientos no jerárquicos suelen exigir que el usuario 
especifique este parámetro antes de que la agrupación se lleve a cabo y los métodos 
jerárquicos habitualmente producen una serie de soluciones que van desde n grupos 
hasta una solución con un único clúster presente (asumir n objetos en el conjunto de 
datos). Cuando aplicamos para los resultados métodos de agrupación jerárquicos, las 
técnicas para la determinación del número de grupos en un conjunto de datos son 
muchas veces referidas como reglas de decisión [5]. 
El dilema de una regla de decisión, que en inglés se denomina comúnmente 
“stopping rule”, es clave ya que en su solución descansa la decisión correcta sobre 
nuestra estructura grupal [6]. 
Algunos procedimientos propuestos presentan problemas, ya que sugieren reglas 
de decisión no automáticas por lo que no eliminan el tema de la subjetividad humana, 
otros son métodos gráficos que requieren el juicio del investigador y en otros casos 
son índices con parámetros de control que no han sido totalmente definidos o 
desarrollados. Otra deficiencia presente en algunas reglas de decisión es su 
incapacidad de operar cuando el agrupamiento óptimo resulta ser k=1 (los n 
elementos deben pertenecer a un único clúster) o k=n (cada uno de los n elementos 
debe permanecer en un clúster individualmente). Y por último, otra dificultad que 
presentan métodos anteriormente propuestos, es el nivel de cómputo asociado al 
procedimiento. 
Por tanto, el objetivo del presente trabajo es proponer un método sencillo, que 
incluya una regla de decisión, para la partición vertical de bases de datos; que 
garantice un índice preciso, que elimine la subjetividad humana del proceso, que no 
sea susceptible a los casos extremos de k=1 y k=n, anteriormente explicados, con un 
nivel de cómputo aceptable, que iguale y/o mejore los resultados obtenidos mediante 
el métodos anteriores. También se presenta una variante de dicho método para que 
pueda ser usado como evaluador de particiones. 
2   Metodología 
Como la inmensa mayoría de los algoritmos previos para la partición vertical de bases 
de datos se usará la Matriz de Uso de Atributos (MUA) como entrada. Esta matriz 
relaciona las transacciones con los atributos de la relación y contendrá un 1 en una de 
sus celdas si el atributo Ai es utilizado en la Transacción Tj o un cero en caso 
contrario. Esta matriz cuenta con una última columna reservada para  las frecuencias 
de acceso de cada transacción, es decir, el número de veces que la transacción se 
solicita en un intervalo de tiempo definido. Se utilizará un ejemplo que considera 8 
transacciones y 10 atributos que producen la MUA que se aprecia en la Tabla I, este 
ejemplo es utilizado en [7][8][9]. 
Tabla 1.  Ejemplo de Matriz de Uso de Atributos de dimensión 10x8.  
ref A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 acc 
T1 1 0 0 0 1 0 1 0 0 0 25 
T2 0 1 1 0 0 0 0 1 1 0 50 
T3 0 0 0 1 0 1 0 0 0 1 25 
T4 0 1 0 0 0 0 1 1 0 0 35 
T5 1 1 1 0 1 0 1 1 1 0 25 
T6 1 0 0 0 1 0 0 0 0 0 25 
T7 0 0 1 0 0 0 0 0 1 0 25 
T8 0 0 1 1 0 1 0 0 1 1 15 
 
Hoffer y Severance en [10] proponen el concepto de afinidad entre pares de 
atributos. Aplicando este concepto a la MUA se obtiene la Matriz de Afinidad entre 
atributos (MAA), que es lo que se propone en los 2 primeros pasos del Método 
Navathe como se indica en [11]. Sin embargo, muchos autores han criticado el uso de 
esta matriz. Sharma Chakravarthy, Jaykumar Muthuraj, Ravi Varadarajan y Shamkant 
B. Navathe en [8] aseguran que, debido a que solo un par de atributos son 
involucrados, esta medida no refleja la cercanía o afinidad cuando más de dos 
atributos son implicados. Aunque en este trabajo se comparte el enfoque de Jun Du, 
Ken Barker y Reda Alhajj, quienes fundamentan en [7] las limitaciones de la medida 
de afinidad como una medida de afinidad local y la necesidad de una medida de 
afinidad global para lograr que todos los valores de la matriz sean comparables entre 
sí. No obstante, para esta investigación, no se considera consistente la medida de 
afinidad global descrita en el trabajo de estos autores, por lo que se realizó un análisis 
de varias de las medidas de afinidad existentes y que son revisadas por el Doctor en 
Ciencias Biológicas Alejandro Herrera Moreno en [6]. Se decide que el Índice de 
Jaccard, una expresión de similitud, es una medida de afinidad global apropiada para 
el tema de la partición vertical de bases de datos, eliminando de su fórmula el factor 
que representa las ausencias conjuntas o ceros compartidos, debido a que el hecho de 
que en una transacción no se usen ninguno de los dos atributos del par analizado, no 
brinda ninguna información para el caso que ocupa. A continuación la ecuación del 
Índice de Jaccard que se sugiere: 
 
S = a/(a + b + c) . (1) 
 
donde: 
S: valor de atracción entre el atributo Ai y Aj, 
a: suma de las frecuencias de aquellas transacciones que usan tanto el atributo Ai 
como el  Aj, 
b: suma de las frecuencias de aquellas transacciones que usan el atributo Ai y no el 
Aj y 
c: suma de las frecuencias de aquellas transacciones que usan el atributo Aj y no el 
Ai. 
Al aplicar la medida de afinidad global seleccionada, se obtiene una nueva matriz, 
se nombrará Matriz de Atracción entre Atributos (MAA*).  
Tabla 2.  Ejemplo de Matriz de Atracción entre Atributos de dimensión 10x10.  
Sij A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 
A1 1 0,156 0,15 0 1 0 0,45 0,156 0,15 0 
A2 0,156 1 0,5 0 0,156 0 0,4 1 0,5 0 
A3 0,15 0,5 1 0,107 0,15 0,107 0,142 0,5 1 0,107 
A4 0 0 0,107 1 0 1 0 0 0,107 1 
A5 1 0,156 0,15 0 1 0 0,45 0,156 0,15 0 
A6 0 0 0,107 1 0 1 0 0 0,107 1 
A7 0,45 0,4 0,142 0 0,45 0 1 0,4 0,142 0 
A8 0,156 1 0,5 0 0,156 0 0,4 1 0,5 0 
A9 0,15 0,5 1 0,107 0,15 0,107 0,142 0,5 1 0,107 
A10 0 0 0,107 1 0 1 0 0 0,107 1 
 
Como se puede apreciar, la matriz es simétrica, los valores quedan normalizados 
entre cero y uno, donde uno representa el valor máximo de similitud y cero el 
mínimo, por lo que todos los valores son comparables entre sí. Es obvio que la 
diagonal principal esté formada por unos, debido a que la atracción de un atributo 
consigo mismo es máxima. 
A partir de este punto se comienza a aplicar el método de agrupamiento jerárquico 
que propone Ward (1963). Recordemos que este procedimiento parte de que los k 
atributos estén individualmente en un grupo y en cada etapa reduce el número de 
grupos en 1, mediante fusiones, hasta que todos los atributos se encuentren en un 
único clúster. Para esta altura, se habrán obtenido k-1 posibles agrupaciones. Tras 
cada fusión deben ser recalculados los valores de “atracción” entre grupos-atributos o 
pares de atributos que provoca el nuevo grupo recién formado. 
Lance y Williams. Boesch (1977) citan ocho estrategias aglomerativas que pueden 
ser utilizadas con este propósito, entre las que se encuentran: ligamiento simple o 
vecino más cercano, ligamiento completo o vecino más lejano, promedio simple, 
promedio de grupos y estrategia flexible. El “promedio simple” es un método que se 
considera como conservativo del espacio ya que introduce poca distorsión en las 
afinidades originales, propiedad que la hace una estrategia muy recomendada [6]. Por 
tal motivo esta es la estrategia seleccionada para el presente método. A continuación, 
en la Tabla 3, se presenta en resumen los resultados obtenidos al aplicar los procesos 
antes mencionados al ejemplo desarrollado. 
Tabla 3.  Resultados de aplicar el método de agrupamiento de Ward con la estrategia 
aglomerativa “Promedio Simple”.  
#  de 
fusiones 
#  de 
grupos 
Grupos Valores de fusión 
0 10 (A1)(A2)(A3)(A4)(A5)(A6)(A7)(A8)(A9)(A10) - 
1 9 (A1, A5)(A2)(A3)(A4)(A6)(A7)(A8)(A9)(A10) 1 
2 8 (A1, A5)(A2, A8)(A3) (A4)(A6)(A7)(A9)(A10) 1 
3 7 (A1, A5)(A2, A8)(A3, A9)(A4)(A6)(A7)(A10) 1 
4 6 (A1, A5)(A2, A8)(A3, A9)(A4, A6)(A7)(A10) 1 
5 5 (A1, A5) (A2, A8) (A3, A9) (A4, A6, A10) (A7) 1 
6 4 (A1, A5)(A2, A8, A3, A9) (A4, A6, A10) (A7) 0,5 
7 3 (A1, A5, A7) (A2, A8, A3, A9) (A4, A6, A10)  0,45 
8 2 (A1, A5, A7, A2, A8, A3, A9) (A4, A6, A10) 0,212 
9 1 (A1, A5, A7, A2, A8, A3, A9, A4, A6, A10) 0,02675 
 
Nótese que los valores de fusión decrecen a medida que aumenta el número de 
fusiones debido a que siempre se selecciona el mayor valor de “atracción” para 
realizar la fusión. El principal problema a enfrentar ahora es escoger cuál de estos 
agrupamientos parece ser mejor, aquí es donde juega un papel fundamental la “regla 
de decisión”. 
Muchos intentos por definir qué es un grupo emplean propiedades como la 
cohesión interna y el aislamiento externo lo cual está más cerca de la definición de 
clasificación que pretende de manera objetiva crear grupos muy homogéneos entre sí 
y bien diferentes de otros. Esto es lo que nos dicen Hair, Anderson, Tatham y Black 
en [12] cuando explican que los grupos deben poseer una homogeneidad interna muy 
alta (“withincluster”) y una heterogeneidad externa (“betweencluster”) también muy 
alta [6]. 
Basado en estos principios en el presente artículo se proponen varias ecuaciones 
para calcular la Homogeneidad Interna (HI) y la Heterogeneidad Externa (HE). A 
continuación se muestra la fórmula para calcular la HI de un grupo específico: 
 
𝐻𝐻𝐻𝐻𝑙𝑙 = �
1 𝑠𝑠𝑠𝑠 𝑛𝑛 = 1
∑𝐴𝐴𝑠𝑠𝑖𝑖
𝑛𝑛(𝑛𝑛 − 1)
2�
