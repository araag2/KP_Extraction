Listas de clusters usando centros espacialmente dispersos para búsquedas por similitud en espacios métricos
﻿ Sparse Spatial Selection is a new pivot—based structure for similarity search in metric spaces. It
is a array—type structure which has shown a good search performance when compared with other selection
methods.

This work describes the building of a new metric structure. It is a tree—type structure and born as a
recursive application from Cluster List and using SSS as a general method for center selection. Preliminaries
experimental results show that it presents a better performance, in terms of the number of distance
evaluations than original Cluster List and other well—known structures.

Keywords: databases, data estructures, algorithms, metric spaces, similarity queries.

Resumen El Sparse Spatial Selection es una nueva estructura basada en pivotes para busqueda por
similaridad en espacios métricos. Esta estructura es del tipo arreglo y ha demostrado buen rendimiento
durante la busqueda comparado con otros métodos de seleccion.

El presente trabajo describe la construccion de una nueva estructura métrica. Esta es una estructura de
tipo arbol y nace de la aplicacion recursiva de Listas de Clusters usando SSS como un método general para
la seleccion de centros. Resultados experimentales preliminares demuestran que tiene mejor desempeﬁo,
en términos de evaluaciones de distancia, que la estrutura Lista de Clusters original y otras estructuras
conocidas.

Palabras claves: bases de datos, estructuras de datos, algoritmos, espacios métricos, consultas por
similaridad.

1 . Introduccién

1.1. Antecedentes

Uno de los problemas de gran interés en ciencias de la computacién es el de ”b1’1squeda por
similaridad”, es decir, encontrar los elementos de un conjunto mas similares a una muestra. Esta
busqueda es necesaria en multiples aplicaciones, como ser en reconocimiento de VOZ e imagen,
compresién de Video, genética, mineria de datos, recuperacién de informacién, etc. En Casi todas las
aplicaciones la evaluacién de la similaridad entre dos elementos es Cara, por lo que usualmente se trata
como medida del costo de la busqueda la cantidad de similaridades que se evaluan.

Interesa el caso donde la similaridad describe un espacio métrico, es decir, esta modelada por una
funcién de distancia que respeta la desigualdad triangular. En este caso, el problema mas comun y
dificil es en aquellos espacios de ”alta dimension” donde el histograma de distancias es concentrado, es
decir, todos los objetos estan mas 0 menos a la misma distancia unos de otros.

El aumento de tamaﬁo de las bases de datos y la aparicién de nuevos tipos de datos sobre los
cuales no interesa realizar busquedas exactas, crean la necesidad de plantear nuevas estructuras para
busqueda por similaridad 0 busqueda aproximada, esto en busca de superar los problemas de las

* Parcialmente ﬁnanciado por los proyectos Fondecyt 1060776, Conicyt; programa de investigacion PR—F1—002IC—06,
Universidad de Magallanes, Chile.

Congreso General 1724

XIII Congreso Argentino de Ciencias de la Computacién

10.

11.

12.

13.

14.

15.

16.

17.

. Sergei Brin. Near neighbor search in large metric spaces. In the 21st VLDB Conference, pages 574 584. Morgan

Kaufmann Publishers, 1995.

N. Brisaboa, A. Fariﬁa, O. Pedreira, and N. Reyes. Seleccion espacial de pivotes dispersos para la busqueda por
similitud en espacios métricos. In XII Congreso Argentino de Ciencias de la Computacio'n, Oct. 2006. San Luis,
Argentina.

W. Burkhard and R. Keller. Some approaches to best—match ﬁle searching. Communication of ACM, 16(4):230 236,
1973.

B. Bustos, G. Navarro, and E. Chavez. Pivot selection techniques for proximity search in metric spaces. In XXI
Conference of the Chilean Computer Science Society, pages 33 40. SCCC, IEEE Computer Science Press, 2001.

E. Chavez, J. Marroquin, and R. Baeza—Yates. Spaghettis: An array based algorithm for similarity queries in metric
spaces. In 6th International Symposium on String Processing and Information Retrieval {SPIRE’99}, pages 38 46.
IEEE CS Press, 1999.

E. Chavez, J. Marroquin, and G. Navarro. Fixed queries array: A fast and economical data structure for proximity
searching. Multimedia Tools and Applications, 14(2):113 135, 2001.

E. Chavez and G. Navarro. An effective clustering algorithm to index high dimensional metric spaces. In The
7th International Symposium on String Processing and Information Retrieval {SPIRE’2000}, pages 75 86. IEEE CS
Press, 2000.

Edgar Chavez, Gonzalo Navarro, Ricardo Baeza—Yates, and José L. Marroquin. Searching in metric spaces. In ACM
Computing Surveys, pages 33(3):273 321, September 2001.

P. Ciaccia, M. Patella, and P. Zezula. M—tree : An efficient access method for similarity search in metric spaces. In
the 23st International Conference on VLDB, pages 426 435, 1997.

L. Mico, J. Oncina, and E. Vidal. A new version of the nearest—neighbor approximating and eliminating search (aesa)
with linear preprocessing—time and memory requirements. Pattern Recognition Letters, 15:9 17, 1994.

Gonzalo Navarro. Searching in metric spaces by spatial approximation. The Very Large Databases Journal {VLDBJ),
11(1):28 46, 2002.

Oscar Pedreira and Nieves R. Brisaboa. Spatial selection of sparse pivots for similarity search in metric spaces. In
SOFSEM 2007: 33rd Conference on Current Trends in Theory and Practice of Computer Science, volume 4362 of
Lecture Notes in Computer Science, pages 434 445, Harrachov, Czech Republic, January, 20-26 2007. Springer.
Caetano Traina, Agma Traina, Bernhard Seeger, and Christos Faloutsos. Slim—trees: High performance metric trees
minimizing overlap between nodes. In VII International Conference on Extending Database Technology, pages 51 61,
2000.

J. Uhlmann. Satisfying general proximity/similarity queries with metric trees. In Information Processing Letters,
pages 402175 179, 1991.

Roberto Uribe—Paredes. Manipulacion de estructuras métricas en memoria secundaria. Master’s thesis, Facultad de
Ciencias Fisicas y Matematicas, Universidad de Chile, Santiago, Chile, Abril 2005.

P. Yianilos. Data structures and algoritms for nearest neighbor search in general metric spaces. In 4th ACM—SIAM
Symposium on Discrete Algorithms {SODA’93}, pages 311 321, 1993.

Congreso General

1733

XIII Congreso Argentino de Ciencias de la Computacién

estructuras existentes hasta ahora. Asimismo, se necesita que dichas estructuras sean dinamicas, es
decir, que permitan agregar o eliminar elementos sin necesidad de crearlas nuevamente. Asi también, las
aplicaciones reales requieren que dichas estructuras permitan ser almacenadas en memoria secundaria
eﬁcientemente, Como también que posean métodos optimizados para reducir los costos de accesos a
disco. Adicionalmente, el método elegido en las estructuras para la eleccion de centros y pivotes resulta
relevante al momento de descartar elementos durante la bfisqueda.

1.2. Marco teérico

La similaridad se modeliza en muchos casos interesantes a través de un espacio métrico, y la
bfisqueda de objetos mas similares a través de una bfisqueda por rango o de vecinos mas cercanos.

Deﬁnicién 1 (Espacios Métric0s): Un espacio métrico es un conjunto X con una funcion de
distancia d: X2 —> R, tal que V:t,y, 2 E X,

1. d(3c, y) 2 0 and d(x, y) = 0 352' IL‘ = y. {positividad)
2. d(:t,y) = d(y, {Simetri’a)
3. d(:t,y) —I— d(y,2’) Z (d(:t,2’). {Desigualdad Triangular)

Deﬁnicién 2 (Consulta por Rang0): Sea un espacio métrico (X,d}, un conjunto de datos ﬁnito
Y Q X, una consulta St E X, y un rango r E R. La consulta de rango alrededor de rt con rango r
es el conjunto de puntos y E Y, tal que d(:t,y) S r.

Deﬁnicién 3 (Los k: Vecinos mds C'ercan0s): Sea un espacio métrico (X,d}, un conjunto de datos
ﬁnito Y Q X, una consulta St E X y un entero k. Los /4: vecinos mas cercanos a rt son un subconjunto
A de objetos de Y, donde la [A] = k y no existe un objeto y E A tal que d{y,rt) sea menor a la
distancia de algfin objeto de A a rt.

El objetivo de los algoritmos de bfisqueda es minimizar la cantidad de evaluaciones de distancia
realizadas para resolver la consulta. Los métodos para buscar en espacios métricos se basan
principalmente en dividir el espacio empleando la distancia a uno o mas objetos seleccionados. El
no trabajar con las caracteristicas particulares de cada aplicacion tiene la ventaja de ser mas general,
pues los algoritmos funcionan con cualquier tipo de objeto 

Existen distintas estructuras para buscar en espacios métricos, las cuales pueden ocupar funciones
discretas o continuas de distancia. Algunos son BKTree [4], MetricTree [15], GNAT [2], VpTree [17],
FQTree [1], MTree [10], SAT [12], Slin1—Tree [14], EGNAT [16].

Algunas de las estructuras anteriores basan la bfisqueda en pivotes y otras en clustering. En el
primer caso se seleccionan pivotes del conjunto de datos y se precalculan las distancias entre los
elementos y los pivotes. Cuando se realiza una consulta, se calcula la distancia de la consulta a los
pivotes y se usa la desigualdad triangular para descartar candidatos.

Los algoritmos basados en clustering dividen el espacio en areas, donde cada area tiene un centro. Se
almacena alguna informacion sobre el area que permita descartar toda el area mediante solo comparar
la consulta con su centro. Los algoritmos de clustering son los mejores para espacios de alta dimension,
que es el problema mas dificil en la practica.

Existen dos criterios para delimitar las areas en las estructuras basadas en clustering, hiperplanos y
radio cobertor (covering radius). El primero divide el espacio en particiones de Voronoi y determina el
hiperplano al cual pertenece la consulta segfin a que centro corresponde. El criterio de radio cobertor
divide el espacio en esferas que pueden intersectarse y una consulta puede pertenecer a mas de una
esfera.

Deﬁnicién 4 (Diagrama de V0r0n0i): Considérese un conjunto de puntos
{C1,c2,...,cn}(centros). Se deﬁne el diagrama de Voronoi Como la subdivision del plano en
n areas, una por cada Ci, tal que q 6 al area Ci si y solo si la distancia euclidiana d(q, Ci) < d(q, Cj)
para cada cj, con j 75 i.

Congreso General 1725

XIII Congreso Argentino de Ciencias de la Computacién

Uno de los problemas que provoca que muchas Veces buenas estructuras arrojen malos resultados es la
eleccion poco afortunada de centros o pivotes. En este sentido, este trabajo propone el uso de un nuevo
método denominado Sparse Spatial Selection (SSS) [3,13], Como un método general para seleccion de
centros o pivotes. Para la aplicacion de este método, se eligio inicialmente la estructura denominada
Lista de Clusters [8], que es de las estructuras basadas en clustering, del tipo arreglo y utiliza el radio
cobertor para descartar centros durante la bfisqueda.

Para este articulo se selecciono, para la realizacion de las pruebas, un espacio métrico consistente en
un diccionario de palabras en castellano de 86.061 objetos, donde la distancia utilizada es la distancia
de edician, la cual entrega Como resultado el n1’1mero minimo de inserciones, eliminaciones o reemplazos
de caracteres, necesarios, para que una palabra sea igual a otra. El Segundo es un espacio de Vectores
de coordenadas reales de dimension 10 generados con distribucion de Gauss con media 1 y Varianza
0.1 cuya cantidad de objetos es de 100,000, para este espacio se utilizo la distancia Euclidiana. Para
la bfisqueda se creo la estructura con el 90 % de los datos y se reservo el 10 % Como consultas.

2. Seleccion de Pivotes y Centros

En particular, la eleccion de pivotes o centros segfin sea la estructura resulta relevante para obtener
un mayor rendimiento durante la bfisqueda, lo que queda demostrado empiricamente en 

Distintas estrategias han sido propuestas Como adecuadas para la eleccion de pivotes. En [11] se
propone seleccionar Como pivotes aquellos objetos que maximicen la suma de las distancias a los pivotes
ya seleccionados. En [2,17] se siguen heuristicas que tratan de seleccionar pivotes que estén lejanos entre
si. En [5] se presenta un criterio de comparacion de la eﬁciencia entre dos conjuntos de pivotes, asi
también se presentan Varias estrategias de seleccion de conjuntos donde se usa el criterio de eﬁciencia
anterior.

El método denominado Sparse Spatial Selection (SSS), es una nueva técnica propuesta para la
seleccion de pivotes, ésta fue originalmente implementado sobre una estructura del tipo arreglo y usa
la desigualdad triangular para discriminar objetos durante la bfisqueda. Dicho método se comporto
igual o mejor en términos de eﬁciencia que los propuestos en [5], con la Ventaja adicional que no se
requiere determinar a priori el n1’1mero de pivotes necesarios.

2.1. Sparse Spatial Selection (SSS)

SSS es un método de seleccion de pivotes en el que escoge un conjunto dinamico de pivotes bien
distribuidos en el espacio, lo que permite descartar mas objetos al momento de realizar una bfisqueda.

Sea (X, d) un espacio métrico, U C X una coleccion de objetos y M la distancia entre los dos
objetos mas alejados. En un principio el conjunto de pivotes esta formado por el primer objeto de la
coleccion. Luego, para cada elemento de la coleccion se Veriﬁca si esta a una distancia mayor o igual a
M >x< oz de los pivotes seleccionados, si es asi, se agrega al conjunto de pivotes, siendo oz una constante
cuyos Valores estan cercanos a 0,4[13]. La ﬁgura 1 muestra la obtencion de los pivotes en un espacio
cualquiera.

 

Figura 1. Los objetos encerrados en circulos representan los centros seleccionados Como pivotes y M
se deﬁne Como la distancia entre Obj 1 y Obj 2, los mas alejados en el espacio.

Congreso General 1726

XIII Congreso Argentino de Ciencias de la Computacién

En [13] la estructura original es basada en pivotes y del tipo arreglo. Su construccion es similar
a las estructuras FQA[7] y Spa_qhett2's[6], pero se diferencia en la forma de elegir los pivotes y en la
manera de buscar. Basicamente se tiene un arreglo donde la cantidad de ﬁlas es la cantidad total de
objetos en la Base de Datos y la cantidad de columnas es el numero de pivotes.

En el presente trabajo se considera que el SSS es basicamente un método de seleccion de pivotes,
por lo que puede ser aplicado a cualquier estructura, independiente del tipo y de los criterios para
delimitar areas. Se considera también, que es posible construir una estructura plenamente basada en
el SSS, es decir, una estructura que puede ajustarse completamente al espacio metrico sobre el cual es
implementada.

3. Lista de Clusters

La Lista de Clusters [8]es una estructura basada en Clustering o particiones compactas, la cual
es muy similar a una Lista Enlazada. Diseﬁada para tener un buen desempeﬁo en espacios de altas
dimensiones.

En la Llsta de Clusters se selecciona un centro c perteneciente a la base de datos X y un radio r
el cual determina la fraccion del espacio que abarca la esfera (C, r) deﬁnida como el subconjunto de
elementos de X los cuales estan a una distancia no mayor a r del centro c. Luego se deﬁne como I a
los elementos que estan dentro de la esfera de centro c tambien llamado Bucket, y E deﬁnido como
el resto de los elementos externos a la esfera de centro c. Este proceso se repite recursivamente. En
consecuencia se obtiene una lista compuesta por un centro, un radio y un Bucket (c,r,I) denominado
Cluster.

Comparado con otros algoritmos de Clustering, la Llsta de Clusters solamente usa el criterio de
radio cobertor y no areas como en el Voronoi—Tree. También es posible Ver la Llsta de Clusters como
un caso particular de Voronio—tree o un M—tree, considerando I y E como los sub—arboles izquierda
y derecha de raiz 0, con las diferencias de que las estructuras recien mencionadas tratan de construir
un arbol balanceado y que ademas poseen estructuras internas, en cambio la Llsta de Clusters es
extremadamente desbalanceada y no poseen estructura interna alguna.

3.1. Construccién

La estructura Llsta de Clusters se construye segun el algoritmo 1, donde U que corresponde a los
datos no insertados en la lista:

Algoritmo 1 C0ntruz'rLC(U)
if U = gzﬁ then
retorna una llsta vacia
end if
Se Selecclona c E U
Se Selecclona un radio 7"
I<—{u€U—c,d(c,u)§r}
E <— U — I
retorna (c, r, I)

La estructura de datos construida deberia ser simétrica, pero no lo es. El primer centro escogido
tiene preferencia sobre los centros subsecuentes por lo que se provoca solapamiento entre clusters. La
ﬁgura 2 lo ilustra. Todos los elementos que estan dentro del cluster del primer centro (cl en la ﬁgura)
se guardan en su Bucket I, a pesar de eso ellos pueden quedar también dentro de los Buckets I de
centros subsecuentes (c2, c3, etc. ﬁgura 2).

Congreso General 1727

XIII Congreso Argentino de Ciencias de la Computacién

    
     

Centre X \
>< C1. .

Radio

>-< X X

  
    
    
 

X
X

Figura 2. Zonas de inﬂuencia de 9 Clusters construidos segun el orden : C1, C2, C3, C4, C5, C6, C7,
C8 y C9.

3.2. Bﬁsqueda

Dado la asimetria de esta estructura, también la busqueda se puede reducir si la consulta esta
totalmente contenida en el Cluster, por lo cual no se necesita considerar E porque por construccién
todos los elementos que estan dentro de la esfera de consulta han sido insertados en I. La busqueda se
muestra en el algoritmo 2, en el cual se aplica una consulta g y un radio de busqueda 7“ sobre la Lista
de Cluster L :

Algoritmo 2 BusquedaLC(L,q,r)

if L 65 vacia then

Se retorna
end if
L = (c,rC,I)
Se Calcula d(c,q)
if d(c,q) g 7" then

0 es un resultado
end if
if d(c,q) g 7°C + 7" then

Se debe buscar eachaustivamente en I
end if
if d(c,q) > Tc — 7" then

Se debe seguir buscando en (E, q, 7")
end if

Esta es una caracteristica esencial ausente en otros algoritmos de Clustering, donde la busqueda
necesita entrar en todos los Clusters que son intersectados por la esfera de consulta. En esta estructura la
busqueda sobre los Cluster restante puede ser cancelada en cuanto la esfera de consulta este totalmente
contenida en un Cluster. En la ﬁgura 3 se puede apreciar tres casos de consultas sobre un Cluster, en
el caso de ql se debe considerar el Bucket actual y el resto de los Clusters, para el caso de q2 se debe
hacer la busqueda sélo en este Bucket y para q3 evitamos la busqueda en el Bucket actual.

Congreso General 1728

XIII Congreso Argentino de Ciencias de la Computacién

rc
centro

®®/

Figura 3. Tres casos de consultas sobre un centro de Cluster.

En las caracteristicas generales de esta estructura no se especiﬁca Como se seleccionan los centros
y radios en cada punto del algoritmo de construccion, ya que esto se relaciona con la eﬁcacia y no a
la exactitud de la estructura de datos. Una buena seleccion de centros podria ser mas bien costosa,
ademas debemos tomar en cuenta que se debe seleccionar un buen radio para cada centro de cluster en
cada punto de la construccion debido a que el espacio Va cambiando en la medida que los objetos son
insertando en la Lista. En este sentido resulta interesante aplicar el método SSS para la seleccion de
centros Versus la opcion de seleccion aleatoria. La estructura posee dos alternativas de construccion,
la primera es seleccionar un radio ﬁjo para todos los Clusters de la lista y la segunda es seleccionar
un tamaﬁo ﬁjo para todos los Clusters de la lista, Cluster de Radio Fljo y Cluster de Tamaﬁo Fljo
respectivamente.

4. Lista de Clusters y Sparse Spatial Selection

Durante la construccion, la Lista de Cluster selecciona inicialmente un centro y un radio, por lo
que resulta natural elegir Como radio M>x<oz y cada centro usando el algoritmo SSS, es decir, los centros
son elegidos si estan ubicados a una distancia M >x< oz de todos los centros anteriores. SSS puede ser
utilizado para radio ﬁjo, Como para tamaﬁo ﬁjo, sin embargo, pruebas preliminares determinaron un
comportamiento superior en listas de cluster con radio ﬁjo. El calculo del Valor de M se realiza sobre
todos los objetos de la base de datos, sin embargo, esto es un proceso off—line y en este trabajo no se
lo considero Como costo de construccion.

La ﬁgura 4 muestra el comportamiento de la estructura durante la busqueda. Los graﬁcos son para
la estructura Lista de Clusters en su Version de radio ﬁjo Versus la alternativa de seleccion de centros
usando SSS y radio M>x<oz. En ambos casos, los graﬁcos corresponden a los mejores Valores encontrados
para radio ﬁjo y a los mejores Valores para oz. Se puede observar que la diferencia es inﬁma, sobre todo
en el caso del espacio de palabras, ello es debido a que la funcion de distancia es discreta, por lo que el
mejor radio, es muy parecido al mejor oz. En el caso del espacio de Vectores, existe una pequeﬁa mejora
al aumentar los rangos de busqueda, en este experimento, el mejor radio es aquel que recupera el 0,1 %
de los datos.

Congreso General 1729

XIII Congreso Argentino de Ciencias de la Computacién

Costo Promedio de Bﬂsqueda (n=77455 palabras) Costo Promedio de Bﬂsqueda (n=90000 vectores, dim 10)
40000 . . 60000 .
A _ ._ ,
35000 _ R d- F-- 3 _ _ _ /_ 55000 - _ ' Radio Fuo 0.1   1 — I ,v’
 Radio M*a|fa?aIi% ollis) — -3 — /  Rad'° M W‘ (‘Na 0-15) * *0 '  '

55 ,' 53 50000 - .  
.9 30000 - / - .9 /C /

D r O ,7

g _/ g 45000 - /r 
:1; 25000 - I’ - w , "

E’ / / S 40000 - »'/ 
.9 /" .9 , .0"

r.) _ // _ o / _ : , /

3 20°00 /=’ 3 35000 - / .    ‘ 15°00 ' Xx’ ' 30000 r  * ’ —
10000 " ' ' 25000 ' '

1 2 3 4 0.01 0.1 1
Rango de Bﬂsqueda Porcentaje recuperado de la Base de Datos
(a) Diccionario Espaﬁol (b) Gauss Dimension 10

Figura 4. Costos promedios de Busqueda Lista de Cluster de Radio Fijo VS Lista de Cluster con SSS

4.1. Lista de Cluster Recursiva

En los graﬁcos mostrados anteriormente no se logra Ver una mejora importante en el desempeﬁo de
la estructura. Esto resulta asi, dado que se seleccionaron los mejores métodos para ambos experimentos.
El mejor radio ﬁjo y oz fueron obtenidos experimentalmente.

Considerando que el espacio es dividido una sola Vez en N partes, es posible aplicar esa division
en los subespacios generados, es decir, cada cluster de la estructura puede ser a su Vez una Lista
de Clusters. Entonces, una segunda alternativa de construccion es aplicar recursivamente el proceso
de construccion sobre cada cluster formado en la estructura original. Para la construccion de esta
estructura se utiliza el método SSS.

Finalmente, lo que se obtiene es una estructura de tipo arbol donde se seleccionan los centros
espacialmente dispersos, usando un radio de M >x< oz.

Cada cluster de la estructura original representa un subespacio con caracteristicas distintas al
original, de hecho los rangos de dicho espacio son mucho menores. Esto puede ser considerado una
Ventaja, ya que al aplicar recursivamente el método SSS usando el M original, el espacio quedaria
sobredimensionado, provocando baja en la eﬁciencia al interior de la estructura. Ahora, es posible
calcular nuevamente el M para el nuevo subespacio, pero implicaria un costo demasiado elevado durante
la construccion. Sin embargo, es posible utilizar el mismo radio cobertor del subespacio para calcular
un M aproximado, sin pagar costos adicionales.

El radio cobertor es la distancia desde el centro a su elemento mas alejado, por lo que se garantiza
que M siempre sera menor o igual a 2 >x< rc (dos Veces el radio cobertor). Esto puede ser utilizado cada
Vez que se realiza el proceso de construccion recursivamente.

La utilizacion recursiva del método SSS sobre cada cluster, modiﬁcando el Valor de M, provoca
que la estructura se Vaya adaptando siempre a la nueva forma del espacio. Lo anterior implica que
la cantidad de nodos en el arbol sera dinamica, es decir, los nodos usualmente no tendran la misma
cantidad de objetos.

Finalmente, el proceso termina cuando el cluster tiene una cantidad de datos inferior a una cota
determinada, por ejemplo a una pagina de disco.

4.2. Resultados Experimentales

En las ﬁguras 5 y 6 se pueden observar resultados interesantes sobre el comportamiento de la nueva
estructura (identiﬁcada Como LGSSSR). En el espacio de Gauss, los Valores graﬁcados corresponden a
los rangos que permiten recuperar el 0,01, el 0,1 y el 1%. La ﬁgura 5 corresponde a mejores Versiones
de Lista de Cluster de radio ﬁjo y Lista de Cluster con SSS Versus Lista de Cluster con SSS y Recursiva

Congreso General 1730

XIII Congreso Argentino de Ciencias de la Computacién

LGSSSR). En este experimento se puede Ver claramente que la nueva estructura tiene una notable
Ventaja sobre las dos Versiones anteriores, siendo dicha Ventaja muy superior en los casos de bajos
rangos de busqueda.

La ﬁgura 6 muestra los resultados para los mismos dos espacios de la nueva estructura Versus tres
estructuras conocidas de buen desempeﬁo en espacios de alta dimensién. De 10s graﬁcos de busqueda
se puede Ver claramente que el nuevo método aventaja a las estructuras MTree, GNAT y EGNAT
muy notoriamente en el espacio de palabras. En el espacio de Gauss el LGSSSR tiene un desempeﬁo
levemente mejor al EGNAT.

La ﬁgura 6 corresponde a un graﬁco comparative entre LC—SSSR y el EGNAT, la mejor de las
estructuras comparadas. El experimento fue realizado sobre una nueva coleccién de datos, la que
corresponde a un conjunto de 47.000 imagenes extraidas de archivos de la NASA y convertidas a
Vectores de 20 componentes. En esta coleccién, se puede notar que la estructura LC—SSSR aun mantiene
una Ventaja importante sobre la estructura EGNAT.

  
         

Costo Promedio de Bﬂsqueda (n=77455 palabras) Costo Promedio de Bﬂsqueda (n=90000 vectores, dim 10)
. . 60000 .
35000 - 55000 _
.9 _ __ Radio Fijo 3 + .0
§ 30000 Radio F||]_OCM;aSIfSaR(a(|fﬁ O61  — —O — — § 50000 —
.. — a a _ — —I — ..
g 25000 _  45000 g % 40000
VJ U) '
§ 20000 - §
-3 E 35000 - Z
3 15000 — 3  
3 . 5 3°°°°   '
‘°°°°‘  ‘ 25000—  —
5000 K ' ' 20000 ’ K '
1 2 3 4 0.01 0.1 1
Rango de Bﬂsqueda Porcentaje recuperado de la Base de Datos
(a) Diccionario Espaﬁol (b) Gauss Dimensién 10

Figura 5. Costos promedios de Busqueda. Mejores Versiones de Lista de Cluster Versus la nueva

estructura.
Costo Promedio de Bﬂsqueda (n=77455 palabras) Costo Promedio de Bﬂsqueda (n=90000 vectores, dim 10)
I I 70000 MTREE I
60000 — MTREE   
GNAT a  T GNAT :1
g EGNAT — —0 —  , <2 00000 _ EGNAT — -0 —  _
§ 50000 _ |_C_SSSR (alfa O_3) — —l —  A // ’ _ § LC—SSSR (alfa 0.35) — —l — 4’ 
0, 40000 —   Q’ — a, 50000 ‘  V D  1:: 0.3’ V, / 0 ‘U  v 5; ’
:1; ,-- / , 5 w , . ,  
E,  A  /, 2  V 07,2
9 30000 - I  ﬁx /// - 9 40000 g , //,;; _
3 »' ,2“ / I 3 , :0 '
0 20000 -   - g _ 
L0    L0 30000— /_//:;,x —
10000—  — 
‘ ’ A ' ' 20000 ” K '
1 2 3 4 0.01 0.1 1
Rango de Bﬂsqueda Porcentaje recuperado de la Base de Datos
(a) Diccionario Espaﬁol (b) Gauss Dimensién 10

Figuraﬁ. Graﬁcos comparativos para la busqueda (MTree V/s GNAT V/s EGNAT V/s LC—SSSR).

Congreso General 1731

XIII Congreso Argentino de Ciencias de la Computacién

Costo Promedio de Bﬂsqueda (n=40700, Vectores Dim 20)
28000 .
LC—SSSR (alfa 0.3) — 1 —

26000 egnat — —0 — V O,//V

I
\
I

I
\
I

24000 .  7’

22000 , 2  —

Evaluaciones de Distancia
I
\

20000 , «  
18000 '
0.01 0.1 1

Range de Busqueda

(a) Espacio de vectores de dimension 20

Figura 7. Graﬁcos comparativos para la busqueda (EGNAT V/S LC—SSSR).

5. Conclusiones

5.1. Aspectos Relevantes y Aportes

Una buena eleccion de pivotes y centros durante la construccion de estructuras métricas siempre
sera relevante para los procesos de busqueda. Considerando que los mejores centros seran dependientes
del espacio, es ideal contar con mecanismos que permitan recolectar, independiente de la forma del
espacio, la mejores alternativas de centros.

En este sentido, los autores consideran que el Inétodo SSS permite, efectivamente, obtener un
conjunto adecuado de centros, lo que queda demostrado claramente en el presente articulo.

Se considera que el principal aporte del presente trabajo es desarrollar la propuesta de una nueva
estructura inicialmente denominada LC—SSSR, basada en la estructura Ltsta de Clusters con aplicacion
resursiva de la seleccion de centros espacialmente esparcidos (SSS). Esta estructura utiliza en forma
natural el Inétodo SSS durante la construccion. Es basada en clustering y del tipo arbol y es competitiva
en espacios de alta dimension.

Los resultados experimentales demuestran lo anterior y proporcionan una Vision de las enormes
Ventajas frente a otras estructuras que son prometedoras.

Es importante mencionar que la nueva estructura se Va adaptando a la forma del espacio, dado el
uso de SSS. Esto también es posible debido al recalculo del Valor de M durante la construccion, lo que
no tiene costos adicionales.

5.2. Trabajos Futuros

Los autores consideran que el desarrollo de esta estructura esta aun en proceso de investigacion
y experimentacion, por lo que se considera entre los trabajos futuros, la comparacion con otras
estructuras, la utilizacion de nuevas colecciones de objetos, la busqueda de los mejores 0/s segun
cada subespacio o nivel interno de la estructura. Segun los autores, una de las tareas pendientes es
disminuir aun mas los calculos de distancia utilizando técnicas de descarte distintas al radio cobertor.
Finalmente, determinar las capacidades dinamicas de la estructura y su comportamiento en memoria
secundaria.

