Validación de un modelo de evaluación de Entornos Virtuales de Enseñanza y Aprendizaje centrado en la usabilidad a partir de su aplicación a un caso de estudio
﻿ En este trabajo presentamos los últimos avances en la construcción 
de un modelo de evaluación de Entornos Virtuales de Enseñanza y de 
Aprendizaje centrado en la usabilidad. El modelo propuesto forma parte de una 
tesis correspondiente a la Maestría de Tecnología Informática Aplicada en 
Educación, perteneciente a la Facultad de Informática de la UNLP. El modelo 
se aplicó al Entorno Virtual de Enseñanza y de Aprendizaje desarrollado y 
utilizado por la Universidad Nacional de Río Cuarto, con el objetivo de analizar 
el comportamiento de la propuesta sobre un producto concreto, dentro de un 
ambiente específico, y rescatar información útil de cara a su mejora y puesta a 
punto. Se presenta una síntesis de los lineamientos generales de las cuatro capas 
que conforman el modelo y aspectos de su aplicación. Finalmente se presentan 
algunos  resultados y líneas de trabajos futuros. 
Palabras clave: EVEA, usabilidad, evaluación. 
1   Introducción 
Los Entornos Virtuales de Enseñanza y Aprendizaje (EVEA) forman parte del 
conjunto de aplicaciones informáticas diseñadas para la utilización de Internet con 
fines educativos. Su principal característica es la interactividad, como estrategia para 
favorecer el contacto entre docentes, alumnos, y materiales de aprendizaje. En 
término generales, suelen ser versátiles para poder adecuarse a diferentes propuestas y 
procurar que el diseño tecnológico acompañe al modelo pedagógico. En realidad, los 
EVEA coadyuvan a la concreción de objetivos educativos al proveer una serie de 
herramientas que facilitan la gestión de usuarios y cursos, y los procesos de 
comunicación, evaluación, colaboración, y distribución de contenidos [1]. Presentan 
una serie de funcionalidades para lograr que los procesos de enseñanza y aprendizaje, 
donde juega algún rol la virtualidad, puedan desenvolverse de la mejor manera. Para 
poder determinar en que medida se logra ese objetivo es necesario realizar una 
evaluación.  
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 661
La evaluación puede orientarse de diferentes maneras, según lo que se pretenda 
evaluar. En general, los modelos de evaluación existentes analizan los EVEA desde el 
punto de vista funcional. Sin embargo, el análisis de las funcionalidades ideales no 
tiene en cuenta la forma en que se puede poner en práctica todo el potencial del 
EVEA. En la actualidad, la mayoría de estos sistemas posee un núcleo común de 
herramientas, con lo cual carece de sentido seguir analizándolos casi exclusivamente 
desde el punto de vista de sus características funcionales [2]. Por tal motivo, se cree 
necesario incorporar una forma de evaluar, donde no se pierdan de vista estas 
características, pero se prioricen otros aspectos centrados en el modo en que los 
usuarios finales toman contacto con la funcionalidad. En este sentido, la usabilidad es 
un concepto muy útil ya que tiene estrecha relación con la utilidad funcional. La 
usabilidad busca analizar cuán bueno es un sistema como para satisfacer todas las 
necesidades y requerimientos de los usuarios [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14].  
Las evaluaciones de usabilidad son factibles de utilizar en diferentes tipos de 
sistemas y distintas fases del proceso de desarrollo de software. Muchos de los 
métodos o técnicas existentes pueden aplicarse en más de una fase, sin embargo, la 
mayoría está destinada a capturar problemas de usabilidad en etapas tempranas del 
desarrollo. Otra forma de analizarla es aquella que se ocupa de evaluarla como un 
atributo del producto final, en lugar de tomarlo en cuenta durante el desarrollo. 
Posiblemente el hecho de centrarse en el contexto de uso sea uno de los rasgos más 
fuertes de esta mirada, ya que pone el acento en la utilización concreta que el usuario 
hace del sistema, analizando en ese contexto las características del mismo. El 
contexto de uso involucra a los tipos de usuarios, las tareas, el equipamiento, y el 
entorno físico y social dentro del cual será utilizado el sistema. Para poder evaluarlo 
es necesario entonces contar con usuarios reales, situaciones reales de trabajo y 
condiciones específicas. Así, el producto deberá estar concluido o en una etapa 
avanzada de desarrollo.  
El modelo de evaluación de usabilidad que se ha propuesto está orientado a una 
evaluación de producto, es decir, sobre EVEAs que ya han sido desarrollados, y están 
en pleno funcionamiento. La evaluación se apoya en escenarios reales de uso, 
teniendo especial consideración por los alumnos y docentes, los objetivos que se 
proponen, las tareas específicas que realizan dentro del entorno durante las 
actividades de enseñanza y aprendizaje, el modelo mental que utilizan, el 
equipamiento e infraestructura que disponen, el lugar físico donde habitualmente se 
desenvuelven y el entorno social en el cual están insertos. El modelo también puede 
facilitar la evaluación de nuevas versiones de un mismo sistema o la comparación 
entre ellas. La idea general está basadas en una estrategia de cuatro niveles o capas de 
evaluación (con una intención específica), que parten de lo general para llegar a lo 
particular, es decir, una estrategia top-down [15]. La primera es la encargada de 
realizar una evaluación del entorno en general, las tres capas restantes se sitúan en un 
contexto de uso particular, por ejemplo, el aula virtual relacionada a un curso. Para 
ello, las capas más cercanas al usuario utilizan escenarios de uso que permiten guiar y 
sistematizar la evaluación. En cada caso, se utilizan métodos y técnicas diferentes que 
se complementan entre sí y posibilitan enriquecer la evaluación. Las capas propuestas 
pueden ajustarse a distintos contextos dependiendo de la necesidad, nivel de 
profundidad de la evaluación, recursos y tiempo disponible. El modelo puede 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 662
adaptarse en dos sentidos, prescindiendo completamente de algunas de las capas, o 
seleccionando diferentes alternativas dentro de la segunda capa. 
En trabajos anteriores se han presentado los fundamentos y características 
generales del modelo, la primera capa de evaluación, los aspectos principales a ser 
considerados para llevar a cabo la segunda capa mediante métodos de inspección, la 
definición de las tareas más importantes y los escenarios de uso, y el núcleo del 
modelo asentado en la segunda, tercera, y cuarta capa de evaluación [2, 16, 17, 18]. 
2   Heurísticas 
Los instrumentos utilizados en la segunda, tercera, y cuarta capa del modelo aplican 
una serie de heurísticas y sub-heurísticas de usabilidad para poder evaluar distintos 
aspectos. Si bien existen principios generales, cada sector suele tener sus propias 
normas o convenciones que se ven reflejadas en la interfaz de sus sitios y en la forma 
de trabajo de sus usuarios. Por tal motivo, las heurísticas están organizadas en dos 
niveles de evaluación, uno general, orientado a la tarea, y otro particular, orientado al 
diseño [19]. Estas heurísticas de usabilidad han sido tomadas a partir de una revisión 
bibliográfica específica [2]. La segunda y la cuarta capa las utilizan todas, mientras 
que la tercera solo las heurísticas orientadas a la tarea. 
- Evaluación de alto nivel orientada a la tarea: examina el aspecto y 
comportamiento desde el punto de vista de las tareas y objetivos propuestos 
por los escenarios de uso. Está compuesta por 4  heurísticas y 17 subheurísticas, aquí nombramos solo las heurísticas: complejidad; visibilidad; 
intuitividad; topografía natural. 
- Evaluación en detalle orientada al diseño: analiza los aspectos concretos 
del conjunto de interfaces provistas por el EVEA para completar las tareas. 
Contempla una mayor cantidad de  heurísticas, 8 en total, junto a 88 subheurísticas (solo se nombran aquí las 8 heurísticas): productividad; 
retroalimentación; control por parte del usuario; reversibilidad y 
manejo del error; diseño y organización; consistencia; ayuda y 
documentación; uso de estándares. 
3   Métricas 
La tercera y cuarta capa incorporan también dentro de sus instrumentos un conjunto 
de métricas de usabilidad para poder evaluar algunos aspectos relacionados con los 
objetivos globales de la tarea y propiedades específicas del EVEA. Cuando se hace 
mención a métricas nos referimos a una medida cuantitativa del grado en que un 
sistema, componente o proceso posee un atributo dado. Las métricas seleccionadas 
poseen directa relación con el estándar ISO 9241-11 [4, 20], y posibilitan medir la 
usabilidad del EVEA sin necesidad de estar involucrados en su desarrollo. 
Eficiencia: Porcentaje de tareas completadas; Efectividad: Tiempo total invertido por 
el usuario en las tareas completadas. Tiempo total ocupado en errores. Tiempo total 
de aprendizaje. Frecuencia de uso de la ayuda o documentación; Satisfacción: Veces 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 663
que el usuario expresa frustración o enojo, utilidad del producto. Satisfacción con 
respecto a las funciones y características. Percepción de que la tecnología da soporte a 
las tareas según las necesidades del usuario. 
3   Capas del modelo de evaluación 
Capa1: está destinada a analizar gran parte de la aceptabilidad práctica del EVEA. 
Ésta conjuga la utilidad (usefullness) con otras variables como costos, compatibilidad, 
confiabilidad, y soporte, entre las más importantes [8, 21]. La utilidad a su vez está 
determinada por la utilidad práctica o funcional (utility), la usabilidad, y 
accesibilidad. En la primera capa se aborda el análisis de todos los componentes de la 
aceptabilidad práctica, menos el referido específicamente a usabilidad. Se analizan 
cuestiones vinculadas a accesibilidad (parte de la utilidad), junto con las 
características técnicas generales (antecedentes, potencial, tecnología utilizada, 
licencia, soporte, seguridad, acceso de usuarios), compatibilidad (servidor, 
usuario/cliente, formatos multimedia, integración) y robustez (integridad de 
funcionamiento, recuperación ante fallos, seguridad). Así, se pone énfasis sobre las 
características funcionales del EVEA, donde se consideran cuestiones relacionadas 
con las facilidades para la organización académica y flexibilidad pedagógica [15]. Un 
punto que toma especial relevancia es la versatilidad del entorno para adaptarse al 
desarrollo de cursos, grupos de cursos, carreras, comunidades virtuales, etc. También 
es importante evaluar la forma en que puede dar soporte a diferentes modalidades 
educativas, sean éstas a distancia o mixtas, y a diferentes enfoques de enseñanza y de 
aprendizaje. Para analizar dichas características es necesario contar con especialistas 
en educación y en tecnología. Esta primera capa de evaluación pone énfasis sobre las 
coincidencias entre una serie de modelos estudiados a partir de una revisión 
bibliográfica específica [15]. Se construye un instrumento de evaluación general, 
centrado en los componentes tecnológicos de los EVEA, pero sin perder de vista los 
componentes organizativos, pedagógicos y didácticos, ya que ambos son 
determinantes al momento de analizar un entorno particular [22]. 
Capa 2: está orientada a evaluar la forma en que el sistema interactúa con el usuario, 
la interfaz que presenta, y el modo en que permite a los usuarios realizar las tareas 
básicas. Los métodos de inspección, como el recorrido cognitivo y la evaluación 
heurística, resultan adecuados para este tipo de acciones [23, 24, 25, 26, 27, 28, 29, 
30]. Las heurísticas de usabilidad utilizadas están organizadas en dos niveles de 
evaluación, uno general, orientado a la tarea, y otro particular, orientado al diseño. El 
primero examina el aspecto y comportamiento desde el punto de vista de las tareas y 
objetivos de los usuarios, y el segundo los aspectos concretos del conjunto de 
interfaces provistas por el EVEA para completar las tareas.  
En un trabajo anterior señalamos que la segunda capa de evaluación requiere de la 
participación de expertos, y métodos de inspección basados en heurísticas y 
recorridos cognitivos para evaluar la forma en que el sistema interactúa con el 
usuario, la interfaz que presenta, y el modo en que permite realizar las tareas básicas 
[17]. Ambos métodos poseen una serie de similitudes respecto a la utilización de 
expertos, escenarios para los distintos roles de usuarios, y preguntas que debe hacerse 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 664
el evaluador durante la aplicación. Producto de éstas coincidencias, fueron 
hermanados para efectuar la evaluación de la interfaz desde ambos puntos de vista, 
pero fundamentalmente desde la evaluación heurística. De esta forma, el experto 
transita una serie de escenarios de uso adoptando distintos personajes, navegando por 
las interfaces respectivas a los fines de determinar el grado en que son respetadas las 
heurísticas, tanto desde el punto de vista de los propósitos como desde los objetivos 
de los usuarios de un EVEA. También advertimos sobre la necesidad de definir 
previamente esos escenarios para permitir describir una situación de uso real, en la 
cual el experto pueda tomar el lugar del usuario final. Para ello se construyeron 21 
escenarios a partir de un contexto de uso determinado, donde los personajes 
principales compuestos por un docente responsable, dos docentes tutores, y tres 
alumnos, realizan las tareas más importantes y frecuentes relacionadas con dicho 
contexto [17]. 
Capa 3: las dos primeras capas focalizan su atención en la mirada que los expertos 
pueden hacer sobre el EVEA. En esta tercera capa el objetivo es nutrirse de la opinión 
del usuario final, pero dentro de un ambiente controlado, y con la participación de 
observadores que guíen y faciliten el proceso. Para ello, es imprescindible la 
participación de usuarios reales. La forma elegida para llevar a cabo esta tercera etapa 
de evaluación es mediante la instrumentación de test de usuarios [31]. El objetivo es 
establecer en qué medida el software se adapta a los estilos de trabajo reales de los 
usuarios, en lugar de forzar a los usuarios a adaptar sus estilos de trabajo al software. 
Dentro de las alternativas existentes para este tipo de métodos, se ha seleccionado una 
variante del test de pensamiento en voz alta denominada test de expresión del usuario 
en base a preguntas [8]. La metodología consiste en brindarle al usuario un escenario 
tipo, y solicitarle que efectúe las tareas involucradas en éste, bajo la atenta mirada de 
un observador. A medida que el usuario interactúa con el EVEA, el observador debe 
realizar preguntas directas acerca del producto o la tarea que el usuario está 
realizando, mientras que éste debe expresar en voz alta sus pensamientos, sensaciones 
y opiniones. El observador debe tomar registro de todo ello para procesarlo una vez 
concluido el test. Para ello distinguimos dos tipos de evaluación: directa e indirecta, la 
primera es realizada por los usuarios, y la segunda por los observadores. Las 
heurísticas utilizadas son aquellas orientada a las tareas, es decir, complejidad, 
visibilidad, intuitividad, y topografía natural. También se incorporan as métricas 
relacionadas con efectividad, eficiencia, y satisfacción. Los usuarios participan de la 
evaluación realizando una o más tareas que forman parte de un escenario tipo. Es 
importante que los usuarios involucrados en los test abarquen los diferentes roles en 
que puede interactuarse con el EVEA. Por otro lado, dentro de cada rol deben 
seleccionarse usuarios con diferente nivel de experiencia en el uso del servicio Web. 
Esta capa es muy costosa en términos de tiempo, cantidad de participantes y 
análisis, puesto que requiere de observaciones directas de distinto tipo de usuarios 
trabajando sobre el entorno. Si bien no existe consenso sobre cuál es la cantidad 
óptima de evaluadores a incorporar, es necesario buscar un equilibrio para que sea 
factible de realizar, 10 como mínimo y 30 como máximo [7, 8]. En tal sentido, para 
obtener un abanico de usuarios que cubra gran parte del espectro deben seleccionarse 
10 usuarios, como mínimo, de los cuales 4 deben ser docentes y 6 alumnos, con el 
objetivo de cubrir los 2 roles de tutor y los 3 de alumno, con al menos dos usuarios 
diferentes cada uno. En la evaluación directa el usuario final debe situarse en cada 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 665
uno de los escenarios e interactuar con el entorno realizando tareas como si estuviera 
participando de un curso o asignatura con modalidad virtual. Para ello, utiliza un 
guión provisto por el observador. Luego de concluida la session, debe completar un 
cuestionario. Para la evaluación indirecta el observador debe estar presente durante 
toda la sesión y registrar en un cuestionario lo que acontece mientras el usuario 
realiza las tareas. El observador debe realizar preguntas directas al usuario, y éste 
debe expresar en voz alta sus observaciones, sugerencias, u opiniones. El usuario 
también puede realizarle preguntas al observador. La evaluación indirecta pretende 
recabar información sobre un conjunto de métricas de usabilidad que intentan medir 
el grado de efectividad, eficiencia y satisfacción del EVEA. Para ello el observador 
debe tomar nota del rol y escenarios que posee cada usuario, comentarios, sugerencias 
u opiniones del mismo, y las métricas. 
Capa 4: está destinada exclusivamente a que los usuarios finales aporten su punto de 
vista dentro de un contexto o ambiente real. Para realizar este tipo de evaluación 
también son adecuados los test de usuarios. En particular, los test remotos son muy 
eficaces, rápidos y fáciles de realizar [32]. Además, el usuario realiza el test en su 
propio medio o ambiente, con lo cual es posible evaluar el contexto de uso. Los test 
remotos se basan, principalmente, en el uso de cuestionarios para recolectar la 
información. Las heurísticas coinciden con las utilizadas en la segunda capa para la 
evaluación orientada a la tarea (complejidad, visibilidad, intuitividad, y topografía 
natural), y al diseño (productividad, retroalimentación, control por parte del usuario, 
reversibilidad y manejo del error, diseño y organización, consistencia, ayuda y 
documentación, y estándares). El evaluador está encargado de suministrar el 
cuestionario a los usuarios reales, examinar las aulas virtuales luego de finalizadas las 
tareas, y posteriormente procesar los resultados. También completa un cuestionario 
que contempla las métricas de efectividad, eficiencia y satisfacción. 
Las dos primeras capas utilizan los expertos para evaluar el EVEA, mientras que la 
tercera y la cuarta hacen lo propio, pero con usuarios reales. En particular, la tercera 
capa involucra a los usuarios a partir de un ambiente controlado, con la participación 
de observadores que guían, facilitan y registran el proceso. Esta cuarta, y última capa, 
hace lo propio, pero dentro de un contexto real de uso, sin laboratorios de usabilidad, 
ni observadores, es decir, en un ambiente totalmente natural donde intervienen otras 
variables como la ubicación geográfica, puntos de acceso, tipos de equipamiento y 
acceso a la red, tipos de software y versiones, además de las tenidas en cuenta en la 
tercera capa.  
Al igual que en las capas anteriores es importante que los usuarios involucrados en 
los test abarquen los diferentes roles y posean distintos niveles de experiencia en el 
uso del servicio Web. 
Para brindar flexibilidad al modelo de evaluación es posible aplicar esta última 
capa de dos maneras diferentes. La más sencilla es hacerlo sobre un EVEA que se 
esté utilizando, es decir, basarnos en su uso real, por parte de usuarios alumnos y 
docentes, en uno o varios cursos que tengan cierta semejanza con los escenarios de 
uso definidos anteriormente. La otra alternativa es recrear en el EVEA los escenarios 
que hemos definido, es decir, generar espacios virtuales para contener cursos e invitar 
a participar a docentes y alumnos. En este caso el curso no es real sino un simulacro, 
donde el contenido de aprendizaje no tiene relevancia, pero los participantes son 
alumnos y docentes que trabajarán en sus propios contextos. La evaluación real es una 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 666
alternativa simple para cuando los recursos humanos son escasos, se necesita una 
evaluación más rápida. Además, el EVEA a evaluar ya debe estar en funcionamiento 
dentro de la Institución y contar con cursos adecuados para realizar la evaluación sin 
necesidad de recrear una situación hipotética. Este tipo de evaluación presenta una 
serie desventajas porque en cursos reales pueden no estar presentes algunas tareas. 
Además, la cantidad de evaluadores no es la misma que para una evaluación 
simulada, y tampoco existe garantía de que estén representados todos los tipos de 
usuarios. Para reducir estos riesgos deben seleccionarse cuidadosamente los cursos y 
asegurarse que la cantidad de usuarios sea representativa, tanto en cantidad como en 
experticia. 
3.1 Ajuste del modelo a diferentes contextos 
Cada capa del modelo plantea el acercamiento hacia el usuario de acuerdo al objetivo 
que persigue, siendo la primera la más lejana y la cuarta la más cercana. Las capas 
pueden ajustarse a distintos contextos dependiendo de la necesidad, nivel de 
profundidad de la evaluación, recursos y tiempo disponibles. Puede prescindirse 
completamente de alguna, o utilizarlas todas, además de seleccionar diferentes 
alternativas dentro de la segunda, tercera y cuarta capa. Tampoco es necesario 
comenzar desde la primera. La única capa imprescindible es la capa 2, posiblemente 
la más importante del modelo. Las alternativas planteadas son: Capa 1  Capa 2  
Capa 3  Capa 4; Capa 1  Capa 2  Capa 4; Capa 2  Capa 3  Capa 4; Capa 2 
 Capa 4. Esta última opción, si bien es la que menos recursos demanda, presenta 
riesgos de sesgar la evaluación puesto que se omite el trabajo cara a cara con los 
usuarios finales bajo un ambiente controlado por observadores. 
4   Aplicación del modelo a un caso de estudio 
Se propuso aplicar el modelo de evaluación al EVEA SIAT (Sistema Informático de 
Apoyo a la Teleformación), en el contexto institucional de la Universidad Nacional de 
Río Cuarto (UNRC) para analizar el comportamiento de la propuesta sobre un 
producto concreto. SIAT viene siendo desarrollado por la UNRC desde el año 2001, 
es utilizado por todas las carreras a distancia de la universidad, y como apoyo a las 
asignaturas de carreras presenciales [1]. 
Para la evaluación del EVEA en cuestión se utilizaron las cuatro capas. Para su 
puesta en marcha, se tomó contacto con la Secretaría Académica, la Secretaría de 
Extensión y Desarrollo, las Facultades de Ciencias Económicas, Ciencias Humanas, y 
Ciencias Exactas, Físico-Químicas y Naturales de la Universidad Nacional de Río 
Cuarto (UNRC), y la Facultad de Informática de la Universidad Nacional de La Plata 
(UNLP). A continuación se describen las decisiones tomadas en cada capa. 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 667
Capa 1: la primera capa del modelo fue llevada a delante por los cuatro evaluadores 
previstos en el modelo, dos de los cuales los consideramos dentro de la categoría de 
especialistas en “TIC y educación”, y otros dos especialistas en tecnología. Uno de los 
especialistas en tecnología se encargó de evaluar la flexibilidad tecnológica y 
organizativa, mientras que el restante hizo lo propio con los estándares. Los dos 
especialistas en educación tuvieron a su cargo la evaluación de flexibilidad 
pedagógico/didáctica y organizativa. La flexibilidad organizativa, en un principio, no 
estaba considerada en el modelo como parte de la evaluación del experto en 
tecnología, pero se decidió incluirla para tener otra mirada disciplinar y porque el 
especialista poseía muy buenos conocimientos acerca de las potencialidades 
organizativas del EVEA a evaluar. Una cuestión a resaltar es que el tiempo de 
evaluación se extendió por más de tres meses, mucho más de lo previsto. Otro aspecto 
importante es que el trabajo del experto dedicado a evaluar estándares demandó el 
triple de tiempo que el de los demás dado que la evaluación de estándares es un 
proceso, en general, lento y tedioso. 
Capa 2: en la segunda capa es posible seleccionar la cantidad de evaluadores acorde a 
factores tales como tiempo, disponibilidad y presupuesto. Por este motivo, el modelo 
propone dos alternativas, una sencilla donde interviene un solo evaluador, y otra 
donde pueden intervenir 3 o 5 evaluadores. Para este caso de estudio, se optó por 
seleccionar tres evaluadores para lograr una evaluación que recoja opiniones de más 
de un experto. Los evaluadores seleccionados fueron: uno de la Facultad de Ciencias 
Exactas, Físico-Químicas y Naturales de la UNRC, con experiencia en el uso del 
EVEA a analizar, y dos a la Facultad de Informática de la UNLP, sin experiencia en 
dicho EVEA. A su vez, para el caso de contar con 3 o 5 expertos el modelo propone 
dos alternativas, una simplificada que evalúa el EVEA a nivel de tareas, y otra más 
exhaustiva que se basa en 21 escenarios de uso. En éste caso de seleccionó la versión 
exhaustiva dado que la adopción de roles es más real, y posibilita realizar una suerte 
de "simulación" siguiendo la secuencia de escenarios planteada. Al contar con varios 
evaluadores es posible distribuir todos los personajes entre los mismos. Un cambio 
respecto al modelo original estuvo en la asignación de roles dado que se asignaron 
dos roles a cada evaluador, en lugar de uno, de manera tal que pudieran cubrirse los 
seis roles definidos (1 docente responsable, 2 tutores, y 3 alumnos). Otra diferencia 
radicó en la experiencia de los evaluadores. Se seleccionaron tres docentes con 
conocimientos en EVEA y mucha experiencia en utilización de computadoras y 
servicios de Internet (E). En cambio, el modelo plantea seleccionar un docente sin 
conocimientos en EVEA, pero con algo de experiencia en uso de computadoras y 
servicios de Internet (E-), un docente tipo E, y docente con conocimientos en EVEA, 
en usabilidad y mucha experiencia en utilización computadoras y servicios de Internet 
(E+). Se decidió esto porque el trabajo de evaluación es muy arduo y se apeló a la 
colaboración de otros investigadores cercanos que, aunque no se ajustaban al perfil, 
garantizaban compromiso y objetividad en la tarea. Su categorización se basó en el 
análisis del currículum vitae de cada uno. 
Entre los obstáculos o problemas más importantes que debieron afrontarse en esta 
capa podemos destacar los siguientes: los evaluadores seleccionados no respondieron 
cabalmente al perfil propuesto en el modelo; el modelo no propone una forma para 
determinar el nivel de experticia (E-, E, E+) del evaluador; el tiempo de evaluación se 
triplicó; los expertos seleccionados estaban dispersos en diferentes lugares 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 668
geográficos. Su coordinación fue muy dificultosa y no se pudo realizar una puesta en 
común; el análisis pormenorizado de los escenarios permitió detectar algunos errores 
en la secuenciación de los mismos; resultó engorroso solicitar que se utilicen varios 
navegadores y conexiones a Internet para realizar todos los escenarios; los escenarios 
resultaron ser algo dependiente entre sí (lo que hace un rol en un escenario influye en 
lo que debe hacer otro rol en el mismo). 
Capa 3: los usuarios participantes fueron solicitados a la Dirección de EAD de la 
Facultad de Ciencias Económicas de la UNRC. Se propuso un grupo de seis docentes 
y nueve alumnos.  En el caso de los docentes, se solicitó que 2 fueran novatos, 2 
medios, y 2 avanzados; mientras que en el caso de los alumnos, 3 que fueran novatos, 
3 medios, y 3 avanzados. Se acordaron dos sesiones de trabajo en días y horarios 
diferentes, y se seleccionaron dos laboratorios de usabilidad con características 
diferentes, uno con computadoras nuevas y otro con equipamiento de 10 años de 
antigüedad. Si bien la mayoría manifestó su predisposición a participar, resultó escasa 
la asistencia de los usuarios a cada sesión y complicada su coordinación. De los 
usuarios convocados concurrieron en total dos docentes y dos alumnos, en lugar de 6 
y 9 respectivamente. Cada sesión estuvo a cargo de un observador para llevar el 
registro de lo acontecido durante la evaluación. A pesar de la escasa concurrencia, se 
pudo apreciar que es necesario contar con un observador por cada dos evaluadores, en 
lugar de dos en total. 
Capa 4: la cuarta capa involucra una evaluación bajo condiciones reales de uso, es 
decir, en un contexto establecido y con usuarios reales. Para ello se seleccionó un 
conjunto de usuarios finales pertenecientes al primer año de las tres carreras con 
modalidad a distancia de la Facultad de Ciencias Económicas de la UNRC, que 
estaban utilizando el EVEA, y poseían un contexto similar al establecido para los 
escenarios. Cabe destacar que las materias de primer año son las mismas para 
cualquier carrera. Otra alternativa era una evaluación simulada, pero se optó por la 
real dado que se contaba con aulas virtuales que estaban en pleno funcionamiento, y 
permitían acceder a resultados basados en el contexto real. La evaluación se llevó a 
cabo durante el segundo cuatrimestre de 2010 en las asignaturas Análisis Matemático 
II, Principios de Economía I, e Historia Económica y Social. Cada una disponía de un 
aula virtual dividida en dos comisiones. El test remoto fue realizado, tanto por 
docentes como por alumnos, en su propio lugar de estudio o trabajo, de manera tal 
que fue posible evaluar el contexto de uso. Esta capa exige una cantidad mínima de 
evaluadores. En el caso de la opción simulada, son 42 usuarios, 15 de los cuales son 
docentes y 27 son alumnos. Para el caso de una evaluación real, como la realizada, 
esta cifra se reduce a la mitad, es decir, 7 docentes y 13 alumnos. La cantidad de 
docentes fue inferior a la exigida (4), mientras que la cantidad de alumnos alcanzó a 
cubrir el mínimo (15). Una diferencia importante con el modelo fue que no se realizó 
un proceso de selección de los usuarios para conocer sus características básicas, y 
determinar su nivel de experticia (novato, medio, avanzado) y su contexto general, 
sino que se suministraron los cuestionarios por correo a todos los usuarios que habían 
participado de dichas materias. También se entregaron personalmente a los alumnos 
que concurrieron a la Universidad a un encuentro presencial de consultas. 
Luego se destinaron 2 meses para recibir las respuestas, y finalmente el observador 
realizó un resumen de las opiniones, tomando como base los dos cuestionarios 
realizados por cada usuario, las huellas que dejaron dentro de las aulas virtuales 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 669
durante el proceso de realización de las tareas, el registro de log de cada una, y el 
cuestionario completado por él mismo. Los problemas que se presentaron estuvieron 
relacionados con la imposibilidad de cubrir la cantidad mínima de usuarios docentes, 
el no abordar una selección de usuarios acorde a su nivel de experticia (novato, 
medio, avanzado) y contexto general, y el largo tiempo de espera  para lograr un 
número aceptable de respuestas. También es destacable el aporte realizado por una 
docente evaluadora que propuso modificar la escala de valoración de algunos 
instrumentos para mejorar la evaluación de las sub-heurísticas relacionadas con 
visibilidad, intuitividad, y topografía natural. 
5   Conclusiones y trabajos futuros 
A partir del análisis de los resultados, realizado luego del caso de studio, se pudo 
observar que el modelo se comportó acorde a las expectativas. Sin embargo, hay 
cuestiones por pulir y mejorar a los efectos de dotarlo de mayor capacidad de 
adaptación ante diferentes circunstancias. Algunas de los problemas que necesitan ser 
abordados están relacionados con los tiempos destinados a la evaluación que fueron 
mayores a los esperados, el orden de algunas tareas pertenecientes a los escenarios de 
uso que sufrieron modificaciones, los instrumentos que recibieron críticas en las 
escalas de valoración, y las adaptaciones que debieron realizarse en las capas 3 y 4 
por no contar con la cantidad y calidad de evaluadores necesarios.  Los próximos 
pasos a seguir están relacionados con la revisión de todo el modelo de evaluación, su 
mejora de acuerdo a la información recogida a partir del caso de estudio, y la 
presentación formal del mismo para su efectiva utilización. 
