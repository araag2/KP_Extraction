Máquinas de Vectores Soporte Adaptativas
﻿Se propone un método de clasificación adaptativo capaz de
aprender un concepto y seguir su evolución temporal como consecuencia
de cambios lentos en sistemas evolutivos. Para ello se realiza una modificación del clasificador SVM (máquina de vectores soporte), consistente
en usar múltiples hiperplanos válidos en pequeñas localidades temporales
(ventanas) para realizar la clasificación. A diferencia de otras propuestas de este tipo en la literatura, en este caso se realiza un aprendizaje
de todos los hiperplanos en forma global, minimizando una cantidad
que contiene al error que comete la familia de clasificadores locales más
una medida asociada a la dimensión VC de los mismos. Para conceptos
estacionarios, la misma idea aplicada a localidades en el espacio de características permite obtener resultados comparables a los que proporciona
SVM con kernel gausiano.
1. INTRODUCCIÓN
Muchos problemas de clasificación asociados a sistemas del mundo real varían
con el tiempo. Por ejemplo, un sistema puede variar por razones físicas como
las estaciones del año, o puede ser necesaria su adaptación por cambio en las
expectativas o intereses de los usuarios del mismo. En la mayoría de los casos,
la causa y características de este cambio no se presentan en los datos a analizar
de manera obvia, por lo que el clasificador asociado tiene que ser capaz no sólo
de aprender la relación entrada-salida correcta en cada instante de tiempo sino
además advertir el cambio en el concepto y adaptarse a él.
Habitualmente este problema se trata usando una ventana temporal, suponiendo que el cambio en el concepto a aprender es despreciable dentro de ella
[1]. Si la ventana es de ancho muy grande, dicha suposición en general no es
válida y el tiempo de adaptación del algoritmo resulta excesivo. Por el contrario,
cuando el ancho de la ventana es chico el algoritmo se adapta rápidamente, pero
es más sensible al ruido y se torna impreciso ya que debe aprender la relación
entrada-salida a partir de unos pocos ejemplos. Dentro de esta aproximación al
problema, existen algoritmos que usan un ancho de ventana adaptable [1], donde
se dividen los datos en grupos (“batchs”) y se usa como ventana la cantidad de
grupos óptima. Aún así, es igualmente necesario suponer que el concepto no
cambia dentro de cada grupo de datos. Por otro lado, un enfoque puramente
estadístico del aprendizaje de conceptos evolutivos puede hallarse en [2].
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1476
En este trabajo presentamos una nueva forma de encarar este problema, en
la cual los clasificadores sucesivos varían siguiendo la mutación del concepto
pero su ajuste se realiza de manera global. Es particular, la ventana temporal
de validez de un clasificador puede ser tan chica como se desee (inclusive un sólo
punto), pero los clasificadores se entrenan minimizando una medida global de
error en lugar de ajustarse localmente. Esta filosofía se aplica onsiderando una
adaptación de uno de los métodos de clasificación más potentes y estudiados en
la actualidad, las llamadas ”maquinas de vectores soporte” o SVM por sus siglas
en inglés [3].
En el caso de clasificación estacionaria, la filosofía arriba descripta puede
aplicarse reemplazando las ventanas temporales por vecindades de un punto en el
espacio de características del problema. De esta forma, el clasificador SVM puede
describir fronteras de decisión complejas (no simples hiperplanos) sin necesidad
de apelar al truco del kernel (“kernel trick”)[3] para lograr un clasificador no
lineal. Mostraremos, a través de un ejemplo simple, que con esta implementación
de SVM adaptativo es posible alcanzar resultados equivalentes a SVM con kernel
gausiano.
2. JUSTIFICACIÓN DEL MÉTODO
Supongamos que tenemos un conjunto de datos [(x1, y1) . . . , (xn, yn)], donde
(xi, yi) fue obtenido en el instante t = i y yi = ±1. Definimos:
Clase F : Clase de clasificadores tal que f ∈ F implica que f implementa
una frontera de decisión constituida por hiperplanos que cambian con t. Así,
un punto en un instante dado es clasificado de acuerdo al lado del hiperplano
correspondiente a ese instante en que se encuentra. Note que la dimensión de
Vapnik-Chervonenkis (VC) [4] de F es ∞, ya que si el hiperplano cambia lo
suficiente desde el instante i − 1 al instante i, podrá clasificar bien el punto xi
sin importar dónde se encuentre.
Clase F reducida: Sea f ∈ Fv un clasificador perteneciente a F , tal que el
cambio del hiperplano de un instante al siguiente está acotado por v. Para ser
más precisos, si en el instante i el hiperplano es fi = wi · xi + bi, entonces
f ∈ Fv ⇔ (wi−1 − wi)2 + (bi−1 − bi)2 ≤ v2 ∀i. (1)
Sea Fv′ el conjunto de clasificadores que no cambian más que v′, con v ≤ v′.
Como Fv ⊆ Fv′ , la dimensión VC de Fv ≤ la dimensión VC de Fv′ . Por otro lado,
la dimensión VC de Fv=0 en <n es n + 1, ya que es el conjunto de hiperplanos
que no varían. Es decir, la dimensión VC de Fv crece con v.
De acuerdo a esto, si queremos controlar la complejidad de las funciones
f podemos limitarnos a elegir funciones de Fv para cierto v. O, por la teoría
de regularización, en vez de buscar la función de Fv que minimice cierto error
Err(f, x, y), podemos buscar la función de F que minimice
Err(f, x, y) + C comp(f)
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1477
donde C es una constante que define la importancia relativa de los errores de
clasificación con respecto a la complejidad de la función comp(f) = máxi[(wi−1−
wi)2 + (bi−1 − bi)2].
Alternativamente, podemos hacer el mismo razonamiento definiendo Fv como
la clase de hiperplanos que en promedio se mueven menos que v, con lo que
llegaríamos a minimizar
Err(f, x, y) + C 1n
∑
i
(wi−1 − wi)2 + (bi−1 − bi)2
Por un razonamiento similar al realizado anteriormente, podemos llegar a
la conclusión de que la dimensión VC baja cuando el margen promedio del
hiperplano móvil crece. Definimos entonces la complejidad de f como
comp(f) = 1n
∑
i
w2i +
C2
n
∑
i
(wi−1 − wi)2 + (bi−1 − bi)2
donde C2 indica a qué causa de aumento de la dimensión VC le damos más
importancia.
Definiendo ahora
Err(f, x, y) =
∑
i
máx[0, 1− yi(wixi + bi)],
llegamos al problema
mínC3
∑
i
máx(0, 1−yi(wixi+bi))+ 1n
∑
i
w2i +
C2
n
∑
i
(wi−1−wi)2+(bi−1−bi)2,
que es equivalente a
mínC3
∑
i
ξi + 1n
∑
i
w2i +
C2
n
∑
i
(wi−1 − wi)2 + (bi−1 − bi)2,
sujeto a
ξi ≥ 0
yi(wixi + bi)− 1 + ξi ≥ 0 .
3. SVM ADAPTATIVO
Tal como fuera explicitado más arriba, tenemos n puntos, x1 . . . xn, divididos
en dos clases, con yi = ±1 la clase del punto xi. Definimos ahora Vi como el
conjunto de puntos vecinos a xi y denotamos Ni al número de puntos en Vi.
En lo sucesivo llamaremos Mi∗ a la fila i de la matriz M y M∗j a la columna j
de dicha matriz. Con P ∗Q indicaremos el producto miembro a miembro de las
matrices P y Q; es decir, (P ∗Q)ij = PijQij .
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1478
Partimos del problema
mín
wi,bi
1
2n
n∑
i=1

||wi||2 + C22
∑
j∈Vi
||wi − wj ||2 + (bi − bj)2

+ C3
∑
i
ξi,
con ||w|| = w · w, sujeto a
ξi ≥ 0
yi(wixi + bi)− 1 + ξi ≥ 0.
El correspondiente lagrangeano es:
L = 12n
n∑
i=1

||wi||2 + C22
∑
j∈Vi
||wi − wj ||2 + (bi − bj)2

+ C3
∑
i
ξi
−
∑
i
αi(yi(wixi + bi)− 1 + ξi)−
∑
i
βiξi, (2)
donde αi ≥ 0 y βi ≥ 0.
Tenemos que maximizar L con respecto a los αi y βi y minimizarlo con
respecto a los wi, bi y ξi. En este punto de mínimo, las derivadas con respecto a
las variables primales tienen que ser nulas:
∂L
∂ξi = 0 = C3 − αi − βi,
lo cual implica que
0 ≤ αi ≤ C3.
Por otro lado, teniendo en cuenta que cada ξi está en L multiplicado por C3 −
αi − βi, (2) queda
L = 12n
n∑
i=1

||wi||2 + C22
∑
j∈Vi
||wi − wj ||2 + (bi − bj)2

−
∑
i
αi(yi(wixi+bi)−1).
(3)
En el caso de los wi se tiene
∂L
∂wi = 0 =
1
n

wi + C2
∑
j∈Vi
(wi − wj)

− αiyixi,
donde se ha considerado que si xj es vecino de xi, xi es vecino de xj . Esta
ecuación resulta
1
n

(1 + C2Ni)wi − C2
∑
j∈Vi
wj

 = αiyixi. (4)
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1479
Si definimos la matriz M y el vector z como
Mij =



(1 + C2Ni)/n si i = j
−C2/n si j ∈ Vi
0 caso contrario.
zi = αiyixi
podemos poner (4) en la forma Mw = z, o bien wi = (M−1)i∗z. Reemplazando
en (3) obtenemos
L = 12nα
T ((M−1)2 ∗K)α+ C24nα
T ((M−1QM−1) ∗K)α+ C24n
∑
i
∑
j∈Vi
(bi − bj)2
−αT ((M−1) ∗K)α+
∑
i
αi −
∑
i
αiyibi. (5)
donde hemos definido dos matrices N ×N , Kij = yiyjxixj y Q dada por
Qij =



Ni si i = j
−1 si i es vecino de j
0 caso contrario
En el caso de los bi,
∂L
∂bi = 0 =
C2
n
∑
j∈Vi
(bi − bj)− αiyi,
de lo que obtenemos
C2Ni
n bi −
C2
n
∑
j∈Vi
bj = αiyi. (6)
Definiendo h como:
h =


α1y1
...
αnyn


podemos escribir (6) en la forma
C2
n Qb = h. (7)
Dado que Q es singular,
Q1 =


N1 −
∑
j∈Vi 1...
Nn −
∑
j∈Vn 1

 =


0
...
0


se puede afirmar que
0 = C2n 0b =
C2
n 1Qb = 1h =
∑
i
αiyi.
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1480
3.1. Eliminación de los bi
Es posible eliminar b de (5). La parte que depende de b es
C2
4n
∑
i
∑
j∈Vi
(bi − bj)2 −
∑
i
αiyibi = C24n
∑
i
∑
j∈Vi
(bi − bj)2 − hT b.
Por otro lado,
bTQb =
∑
i
∑
j∈Vi
(bi − bj)bi.
con lo que
C2
4n
∑
i
∑
j∈Vi
(bi − bj)2 − hT b = C22nb
TQb− hT b.
De (7) sabemos que
bTQ = nh
T
C2 ,
con lo cual queda
C2
2n
nhT
C2 b− h
T b = −h
T b
2 .
Diagonalizando Q resulta Q = PDPT , con P−1 = PT ya que Q es simétrica,
con lo cual de (7) podemos obtener
DPT b = PT nhC2
Si definimos b′ = PT b y h′ = PTh, esto es
Db′ = nh
′
C2 .
Como D es diagonal, es fácil encontrar los b′i para los cuales Dii = λi 6= 0:
b′i =
nh′
C2λi , si λi 6= 0.
Si el autovalor λi = 0, de (7) tenemos que
PTi∗Qb = PTi∗
nh
C2
0b = PTi∗
nh
C2
0 = nh
′
i
C2 .
0 = h′i
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1481
Es decir, cuando no podemos determinar b′i por la ecuación anterior, h′i = 0.
Volvamos a lo que queremos obtener:
−h
T b
2 = −
htPPT b
2
= −12h
′T b′
= −12
∑
i
h′ib′i
= −12
∑
i
n
C2Λih
′2
i
donde Λi = 1λi si λi 6= 0 y Λi = 0 si λi = 0. Si se define D′ como la matrizdiagonal D′ii = Λi e Y dada por Yij = yiyj , esto último queda
−h
T b
2 = −
n
2C2h
TPD′PTh
= − n2C2α
T ((PD′PT ) ∗ Y )α.
Reemplazando en (5) obtenemos
L = 12nα
T ((M−1)2 ∗K)α+
C2
4nα
T ((M−1QM−1) ∗K)rrα−
αT ((M−1) ∗K)α+
∑
i
αi −
− n2C2α
T ((PD′PT ) ∗ Y )α
Es decir,
L = αT
[( 1
2nM
−2 + C24nM
−1QM−1 −M−1
)
∗K − n2C2 (PD
′PT ) ∗ Y
]
α+
∑
i
αi,
lo que implica que es de la forma
L = αTRα+
∑
i
αi
con la matriz R definida de manera adecuada.
El problema dual queda entonces
máx
α
αTRα+
∑
i
αi,
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1482
sujeto a
0 ≤ αi ≤ C3∑
αiyi = 0,
que es el problema resuelto en SVM (aunque con otra R). En consecuencia,
para resolverlo se puede usar cualquiera de las técnicas empleadas para SVM
convencional, como por ejemplo SMO[3].
4. EXPERIMENTOS
4.1. Bases de Datos Artificiales
Se probó el método en las siguientes tres bases de datos artificiales:
Dataset 1: Se generó un dataset de puntos xi ∈ <2, divididos en dos clases, la
clase 1 formada por 250 puntos centrados en (0, 1), con un desvío estandar de 0,1
en cada dimensión y la clase −1 formada por 250 puntos centrados en (0,−1),
con el mismo desvío.
Para simular un cambio a través del tiempo, a cada punto xi, representado
en coordenadas polares, se le sumó un ángulo de ipi500 radianes. Al algoritmo se
le presentaron los puntos en coordenadas rectangulares. El resultado se grafica
en la figura 1.
Figura 1. Dataset 1
Dataset 2: A los mismos puntos que en el dataset anterior se les sumó un ángulo
3ipi
1000 en vez de ipi500 . En este caso el solapamiento de las clases es mayor (ver figura
2).
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1483
Figura 2. Dataset 2
Dataset 3: Se tomaron 500 puntos x ∈ <2 al azar con x1 ∈ [−0,2; 1,2] y x2 ∈
[−5; 5], con distribución uniforme. Al punto xi se le asignó la clase 1 si
x2,i ≥ 11 + e−x1,i
y la clase −1 en caso contrario (ver Figura 3).
Figura 3. Dataset 3
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1484
La intención de las pruebas con los dos primeros datasets fue ver cómo evoluciona el clasificador a través del tiempo al variar el parámetro C2 y compararlo
con la solución encontrada por SVM estándar con kernel lineal (que no tiene en
cuenta el tiempo en que cada una de las muestras fue colectada).
Para que el método busque una evolución temporal, se consideró como conjunto de puntos vecino a xi a {xi−1, xi+1} (salvo en el caso de los extremos). Es
decir, se usó una matriz Q dada por
Q11 = Qnn = 1
Qii = 2 para i 6= 1 e i 6= n
Qi,i+1 = Qi,i−1 = −1
Qij = 0 en otros casos.
El parámetro C3 se dejó fijo en 1 mientras se variaba C2. Acorde a esto, se
usó C = 1 para el parámetro de SVM.
Los resultados obtenidos muestran que el clasificador cambia de la misma
forma en que se generaron los puntos para los valores más bajos de C2 y tiende
a la solución encontrada por SVM estándar a medida que crece C2. El error
de entrenamiento es 0, salvo para valores altos de C2, que obligan a que el
clasificador no varíe mucho y en consecuencia no puede seguir la evolución de
los datos (cabe aclarar que los datasets no tienen puntos mal clasificados). En
la figura 4 se ve la evolución del clasificador para C2 ∈ {10, 103, 106, 108}. Las
líneas unen el extremo de cada vector wi con el de los vectores wi+1 y wi−1; es
decir, constituyen la gráfica de la curva w(t). Los errores de entrenamiento para
estos cuatro casos fueron 0, 0, 0 y 2,6%, respectivamente.
Figura 4. Dataset 1. Las líneas muestran la evolución w(t) para C2 = 10 (verde), 103
(azul), 106 (roja) y 108 (negra); el punto negro corresponde al w obtenido con SVM
no adaptivo.
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1485
Para el dataset 2 se obtuvieron resultados similares. En la figura 5 se ven las
curvas correspondientes a C2 ∈ {103, 106, 107, 108}. Los errores de entrenamiento fueron de 0, 0, 0,2% y 28,8% para cada uno de estos valores. Cabe destacar
que el alto valor de error correspondiente a C2 = 108 constituye aproximadamente el resultado esperado para SVM estándar (no adaptativo), debido a la
alta superposición de las clases.
Figura 5. Dataset 2. Las líneas muestran la evolución w(t) para C2 = 10 (verde), 103
(azul), 106 (roja) y 108 (negra); el punto negro corresponde al w obtenido con SVM
no adaptativo.
Con el tercer dataset se ejemplifica cómo se comporta el método en problemas
cuya frontera de decisión no sea un simple hiperplano (tratando esta situación
como un cambio adaptativo en el espacio de características del problema en
lugar del tiempo). Si bien la frontera de decisión es una función compleja de la
entrada, se pueden separar las clases con un hiperplano que varíe lentamente a
medida que cambia una de las coordenadas. Note que en este caso la aplicación
del método se realiza sobre un sistema estacionario, por lo que este ejemplo en
realidad explora la capacidad del algoritmo aqui propuesto de constituirse en
una alternativa al uso de SVM con kernels no-lineales.
Para que el método busque soluciones que varíen localmente se consideró como vecinos a los puntos xi y xj sii xi es uno de los 5 vecinos más próximos a xj o
xj es uno de los 5 vecinos más próximos a xi. Lo mismo que antes, el parámetro
C3 se dejó fijo en 1 y se usó también 1 para el parámetro C de SVM. Como
conjunto de test consideramos una grilla de puntos dentro del mismo rectángulo
que los datos de entrenamiento. A cada punto de test se lo clasificó con el hiperplano asociado a su vecino más próximo dentro del dataset original usado para
aprendizaje del clasificador.
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1486
En las figuras 6-9 se muestran los resultados obtenidos sobre el conjunto de
test para valores de C2 de 103, 104, 5 × 104 y 5 × 105 respectivamente. Como
era esperable, para C2 grande la frontera de decisión obtenida es muy similar a
la que produciría el SVM estacionario con kernel lineal.
Figura 6. Dataset 3. Resultados para C2 = 103.
También se compararon los resultados con los obtenidos con SVM con kernel
gausiano. El reportado en el cuadro 1 es el mejor que se pudo conseguir variando
el parámetro γ de la gausiana y dejando el parámetro C fijo en 1.
Cuadro 1. Errores de test para el dataset 3.
Prueba Error (%)
C2 = 103 3,61
C2 = 104 2,12
C2 = 5× 104 2,81
C2 = 5× 105 5,17
SVM lineal 5,73
SVM gausiano (γ = 2,94) 2,37
Como puede verse en la tabla, nuestro método supera al resultado óptimo de
SVM con kernel gausiano, aunque los resultados tienen que considerarse preliminares dado que para ninguno de los dos métodos se realizó un ajuste exhaustivo
de los parámetros.
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1487
Figura 7. Dataset 3. Resultados para C2 = 104.
Figura 8. Dataset 3. Resultados para C2 = 5× 104.
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1488
Figura 9. Dataset 3. Resultados para C2 = 5× 104.
4.2. Bases de Datos Reales
De los dos primeros ejemplos sintéticos considerados en la sección anterior
surge claramente la capacidad del algoritmo propuesto para resolver problemas
de clasificación adaptativa. Ello no es sorprendente en tanto el método fue específicamente diseñado teniendo en mente este tipo de aplicaciones. Es destacable, en cambio, que el mismo permita resolver problemas de clasificación estándar
con precisión comparable a la de SVM no lineal (Kernel SVM), tal como surge
del análisis del tercer dataset considerado previamente. Para explorar más en
detalle esta capacidad, se probó el método propuesto en un problema real. Para
ello se usó el dataset breast cancer del IDA repository [5]. Al igual que en [6],
se consideraron 100 divisiones de los datos para entrenamiento y test (200 para
el entrenamiento y 77 para test). Para seleccionar los parámetros del método se
hizo en cada una de las 100 pruebas una validación cruzada con 10 “folds”. Se
probó variar el parámetro C2 en {10, 102, 103, 5× 103, 104, 5× 104} y el número
de vecinos usados en {3, 4, 5, 7, 10, 15, 20, 25}.
En el cuadro 2 se muestra el mejor resultado obtenido y el reportado en [6]:1
Cuadro 2. Errores de test para el dataset “breast cancer”.
Prueba Error (%)
Nuestro Método 27,0± 5,7
SVM 26,0± 4,7
1 En [6] el σ reportado para SVM es de 0,47, mientras que en el resumen en [5] figura
4,7.
XIII Congreso Argentino de Ciencias de la Computación
¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯
 
VIII Workshop de Agentes y Sistemas Inteligentes
_________________________________________________________________________
 
 
1489
La pequeña diferencia en los promedios de error, que favorece al mejor resultado obtenido con SVM convencional, no es significativa dada la gran dispersión
observada en dichos promedios. Es decir, nuevamente aqui, sobre un problema
real, se obtiene que SVM adaptativo es comparable a SVM con kernel gausiano.
Esta capacidad del método propuesto constituye un importante valor agregado
del mismo, ya que permite generar un clasificador no lineal en el espacio de características original, independizándonos de la elección de un mapa a un espacio
de mayor dimensión (a través del kernel utilizado).
5. CONCLUSIONES
En el presente trabajo hemos propuesto un nuevo método de generación
de clasificadores adaptativos, capaces de aprender conceptos que mutan con el
tiempo. La idea se ejemplificó desarrollando en detalle una versión local del clasificador SVM, que permite recuperar la evolución temporal de un problema de
clasificación simple. La idea básica consiste en usar múltiples hiperplanos válidos
en pequeñas localidades temporales (ventanas) para realizar la clasificación pero,
a diferencia de otras propuestas de este tipo, realizando un aprendizaje de todos
los hiperplanos en forma global. Para ello se minimiza una cantidad que contiene
al error que comete la familia de clasificadores locales más una medida asociada
a la dimensión de VC de los mismos. Por otro lado, la misma idea aplicada a
localidades en el espacio de características del problema de clasificación permite
obtener resultados comparables a los que proporciona SVM con kernel gausiano.
Los resultados presentados son preliminares y requieren aún una experimentación más exhaustiva para establecer ventajas y desventajas del método
propuesto. No obstante, al menos para sistemas no estacionarios, los mismos
son lo suficientemente prometedores como para ser optimista en cuanto a la
aplicación de esta técnica a problemas reales en sistemas que mutan lentamente.
