Sonificación de los datos generados por ATLAS
﻿  La  sonificación  es  el  uso  del  audio  no  hablado  para  representar 
información o perceptualizar datos. Debido a las características específicas del 
sentido  de  audición,  como  su  resolución  temporal,  de  frecuencia,  de 
localización,  y  de  presión,  la  sonificación  se  presenta  como  una  técnica 
alternativa o complementaria a las técnicas de visualización de datos. Nuestro 
trabajo encara la colaboración con un científico del CERN para sonificar los 
datos  generados  por  ATLAS,  uno  de  los  cuatros  grandes  experimentos 
incluidos en el Large Hadron Collider. Para ello,  utilizamos MAX/MSP, un 
lenguaje  de programación visual  diseñado específicamente para la  síntesis y 
procesamiento de sonido en tiempo real. Las decisiones más importantes tienen 
que ver con la asociación entre los datos del experimento y las cualidades (y 
combinaciones) de los sonidos generados para lograr una percepción efectiva 
por parte del usuario de aquellos eventos relevantes.
Keywords: sonificación, visualización, nuevos medios.
1. Introducción
   "The auditory system is the best pattern recognition device we have. If you're trying  
to  find  patterns  in  complex,  time-varying data,  then listening  to  it  is  much more  
effective than looking at it". Bruce Walker, físico y psicólogo del Georgia Institute of 
Technology, EEUU. Miembro del Sonification Lab.
   Como se menciona en la cita, en ciertas situaciones, el sentido de la audición puede 
ser tan o más eficaz que el sentido de la vista para monitorear, interpretar y navegar 
datos  complejos.  Si  se  usan  sonidos  en  lugar  de  imágenes  como  medio  de 
representación de los datos,  se habla de sonificación en lugar de visualización. Se 
suelen listar ciertas ventajas o beneficios de la sonificación:
• movimiento libres: en lugar de tener que fijar los ojos en una pantalla u otro 
dispositivo de salida gráfica, el usuario puede moverse con mayor libertad
• monitoreo en paralelo con otras tareas: si un cambio drástico en el flujo de 
datos  se  representa  con  un  cambio  drástico  en  el  sonido,  el  sistema  de 
sonificación automáticamente llamará la atención del usuario, quien puede 
dedicarse a otras tareas mientras tanto
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 444
• resolución temporal: la resolución temporal del sistema auditivo es cerca de 
dos veces más alta  (20-30ms) que la resolución temporal de la visión (50-60 
ms).  Si  consideramos  la  localización  espacial,  el  sistema  auditivo  puede 
diferenciar intervalos de tiempo de menos de 1ms.
• rangos dinámicos muy grandes: escuchamos en un gran rango de amplitudes 
y alturas lo que permite una gran resolución de representación de los datos.
• información complementaria: la representación por medio del sonido puede 
ser utilizada en combinación con la visualización clásica.
• estructuras escondidas: como la escucha es simplemente una forma diferente 
de percepción, una técnica de sonificación puede mostrar nuevas estructuras 
en los datos que no eran detectadas por medio de la visualización.
  Por estos y otros motivos, la sonificación es un campo de investigación cuyo estado  
del  arte  se  mueve  rápidamente  y  cada  vez  llama  más  la  atención  de  científicos, 
ingenieros  y  grupos  interdisciplinarios.  Entre  las  implementaciones  recientes  más 
destacadas,  pueden  mencionarse  aquellas  relacionadas  con  la  sonificación  de  la 
actividad volcánica [6] gracias a la colaboración de organizaciones como DANTE, 
dedicada a la coordinación de redes para la investigación (junto a EGEE y EELA), o 
la sonificación de la actividad del viento solar [7] llevada a cabo por investigadores de 
la Universidad de Michigan. La nasa ofrece su propio framework, llamado xSonify 
[8]  y  la  Universidad  de  California  ha  diseñado  uno  de  los  laboratorios  más 
importantes del  mundo, llamado Allosphere [9],  donde la  ingeniería y  los  nuevos 
medios se combinan para lograr nuevos paradigmas de visualización y sonificación de 
datos científicos. Allí se llevan a cabo una gran cantidad de proyectos de realidad 
aumentada y espacios virtuales inmersivos aplicados distintas ramas de la ciencia y la 
tecnología.
   Es importante destacar que la  sonificación diseñada sin cuidado puede generar 
sonidos molestos  y agotadores para el  usuario.  Los intentos  más recientes buscan 
resultados de audio estéticamente aceptables para que la sonificación sea adoptada 
ampliamente como una herramienta en la exploración de datos. Idealmente, debería 
ser algo placentero de escuchar. 
   El interés por las decisiones estéticas en los frameworks de sonificación ha crecido 
considerablemente, hasta volverse uno de los factores fundamentales, como se plantea 
en [1] y [2]. También la interactividad entre el usuario y el sistema de sonificación ha  
cobrado importancia, luego de diversos estudios que han demostrado el incremento de 
la  eficacia  mediante  el  uso  de  estas  técnicas.  Pueden estudiarse  algunas  de  estas 
aproximaciones en [4] y [5].
2. Conceptos introductorios
   El oído humano puede percibir señales periódicas con frecuencias entre los 20 Hz y 
los 20.000 Hz. La fecuencia es detectada en una escala logarítmica con base 2, por lo 
tanto se suele hablar de altura o  pitch.  Un intervalo de una octava  en términos de 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 445
altura tonal significa un incremento del doble de la frecuencia base del intervalo. Es 
posible utilizar mensajes MIDI [13] para comunicar alturas, intensidades y duraciones 
de los sonidos entre los distintos módulos de software y hardware. 
   El timbre es la micro-estructura de un sonido, se caracteriza por las frecuencias que  
componen el espectro de un sonido y por su envolvente dinámica. Define al sonido en 
términos cualitativos.
   La espacialización del sonido es el uso de alguna de las posibles técnicas para  
localizar la fuente del sonido, real o virtual, en un lugar determinado en el espacio, 
también real  o  virtual.  Los avances en espacialización,  debido al  desarrollo  de la 
multimedia,  la  música  electroacústica  y las  tecnologías  del  entretenimieno,  en  los 
últimos años han sido muy importantes, como puede estudiarse en [12].
   A continuación se mencionan brevemente algunos métodos básicos de sonificación, 
según las taxonomías aceptadas [3]. La primer gran distinción que debe hacerse es la 
que separa a la sonificación de los métodos de composición algorítmica utilizados por 
los músicos académicos a partir del siglo XX. La sonificación es la generación de 
sonido a partir de datos sólo si es sistemática, objetiva y reproducible, de manera que 
pueda ser utilizada como un método científico. Cualquier técnica que utilice datos de 
entrada  y genere  (eventualmente  en respuesta  a  algún estímulo  adicional)  señales 
sonoras puede ser llamada sonificación si y sólo si: 
• (A) el sonido refleja las propiedades y/o relaciones de los datos de entrada.
• (B) la transformación es completamente sistemática. Esto significa que hay 
una definición precisa de cómo las interacciones y los datos generan cambios 
en los sonidos.
• (C) la sonificación es  reproducible: dados los mismos datos de entrada y 
estímulos adicionales, el sonido resultante debe ser estructuralmente idéntico
• (D) el sistema puede ser intencionalmente utilizado con diferentes conjuntos 
de datos puede utilizarse repetidamente con el mismo conjunto de datos
   De acuerdo con esta definición, las técnicas de audificación, los audíconos o íconos 
auditivos (earcons), la sonificación basada en modelos y la sonificación basada en el 
mapeo  de  parámetros  son  formas  válidas  de  sonificación.  Todas  representan 
información o datos mediante el uso del sonido de manera estructurada.
   En la audificación los valores de un flujo de datos son directamente interpretados  
como muestras de audio. Los íconos auditivos son eventos sonoros que se utilizan 
para  presentar  información  de  un  sistema,  principalmente,  como  respuesta  a  las 
acciones de un usuario. La sonificación basada en modelos genera sonidos derivados 
de sistemas dinámicos que se parametrizan con el conjunto de datos a ser sonificado. 
Su propósito principal es la exploración de conjunto de datos de muchas dimensiones 
los cuales son difíciles de representar gráficamente. Intearctuando con el modelo, el 
usuario  puede  ganar  un  mayor  entendimiento  de  la  estructura  de  los  datos 
subyacentes.  Finalmente,  la  sonificación  basada en el  mapeo de  parámetros  es  la 
forma más general de sonificación. Un flujo de datos es constantemente mapeado a 
algunos de los parámetros del método de generación de sonido. La audificación puede 
considerarse como el  caso más simple de este tipo de sonificación: el  método de 
generación de sonido sería, en este caso, un mapeo identidad. Los flujos de datos  
modifican los parámetros del método de generación de sonido tales como la altura, la 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 446
amplitud,  el  timbre,  la  localización,  etc,  y  reflejan  de  esta  forma su  contenido  y 
relaciones. Este tipo de sonificación puede realizarse en tiempo real y es ideal para  
nuestro objetivo. El desafío es encontrar una función de transformación eficaz. 
3. Análisis de nuestra implementación
   El  prototipo  presentado  aquí  fue  aceptado  como  proyecto  en  el  programa 
Interactivos 2011 realizado en el Espacio Fundación Telefónica de Buenos Aires y 
continúa  desarrollándose  con  el  asesoramiento  de  los  coordinadores  del  taller. 
También  fue  presentado  en  la  convocatoria  de  proyectos  que  combinan  arte  y 
tecnología, realizada por el Museo de Arte Moderno de Buenos Aires y la Fundación 
Telefónica de Argentina.
   El CERN reune a miles de científicos provenientes de 20 países miembros y otros  
tantos paises colaboradores, y el LHC es uno de los experimentos más importante de 
la  humanidad  hasta  el  momento.  Es  difícil  cuantificar  el  enorme  poder  de  este 
acelerador de 27 km de circunferecia construido a 100 metros bajo tierra. En febrero 
de  2011,  han  conseguido  hacer  colisionar  haces  de  protones  a  una  velocidad  sin 
precedentes:  10.000 colisiones por segundo. De esta forma, los científicos pueden 
darse una idea de qué ocurrió en la milmillonésima de segundo posterior al Big Bang. 
También se espera que el LHC permita finalmente confirmar la existencia del bosón 
de Higgs, que es la única partícula elemental del modelo estándar que no ha sido 
observada experimentalmente aún. Los enormes detectores generan miles de números 
en punto flotante por segundo para ser  analizados por una enorme infraestructura 
distribuida entre diversos paises, bajo un modelo de Grid Computing.
   En 2010, una científica inglesa llamada Lily Asquith propuso la sonificación los  
datos generados por el experimento ATLAS. Lamentablemente no ha habido ninguna 
actualización o mejora del proyecto, luego del impulso inicial [11]. En diciembre del 
año 2010 comenzamos a coolaborar con el proyecto, y a proponer nuevas formas de 
llevarlo aún más lejos. Para esto fue necesario repensar las herramientas y técnicas de  
sonificación utilizadas, teniendo en cuenta que el estado del arte de esta disciplina se  
mueve a  gran  velocidad.  Las  primeras  decisiones tuvieron que ver  con migrar  el 
proyecto a una infraestructura construida en base a MAX/MSP y comenzar a diseñar 
el  mapping entre ciertas magnitudes físicas y la localización de los sonidos en el 
espacio, teniendo en cuenta las nuevas técnicas de espacialización [12].
   Lily Asquith se comprometió a brindarnos la asistencia necesaria y nos envió un 
extenso juego de datos simulados, organizados en archivos con columnas de números. 
Los archivos contienen tres columnas de números. Cada archivo se corresponde con 
un jet (un spray de partículas en el detector ATLAS). Cada fila es una pequeña parte 
del jet (una célula) y las columnas significan lo siguiente: 
1: la distancia entre la célula y el punto de interacción (el punto en el  medio del 
detector donde los protones colisionan). Ésta crece a medida que avanzamos en el jet.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 447
2: la distancia angular entre los ejes de las células (los jets tienen una forma cónica y  
tienen un eje apuntando desde el punto de interacción hacia afuera).
3: la cantidad de energía depositada en la célula, en Mega electronVolts (MeV). El 
umbral es de 30 MeV.
   Nuestra implementación asocia el primer parámetro con la altura tonal del sonido 
generado, el segundo parámetro con la localización espacial en el plano horizontal y 
el  tercer  parámetro con el  timbre del  sonido (altura espectral).  MAX/MSP ofrece 
todas las funcionalidades necesarias y facilita el control del hardware externo multicanal necesario para la instalación del sistema completo. En la imagen se observa la 
arquitectura básica del sistema de sonificación. Los archivos de texto con los datos 
simulados son leidos por el sistema de sonificación para controlar los parámetros de 
generación  de  los  sonidos  sintetizados.  Luego,  el  algoritmo  de  espacialización 
alimenta una instalación cuadrafónica que rodea al usuario.
   
   Los números leidos de la primer columna de los archivos se transforman en alturas  
codificadas  en  mensajes  de  protocolo  MIDI.  Estos  mensajes  son  enviados  a  un 
módulo de síntesis básico implementado para este trabajo. 
   Las variaciones de timbre se logran a través de la técnica de síntesis por frecuencia  
modulada [15] que logra una forma de onda compleja de manera muy eficiente. El 
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 448
parámetro  conocido  como índice  de  modulación  controla  la  riqueza  espectral  del 
sonido en el algoritmo de síntesis y es el que se utiliza para modificar el timbre.
   Con respecto a la espacialización, se utiliza la codificación  ambisonics  [12] para 
alimentar una instalación cuadrafónica, por ser un buen compromiso entre flexibilidad 
y costos. Este tipo de sistema se conoce como pantofónico. El algoritmo ambisonics 
puede ser implementado fácilmente por software para codificar la localización de los 
sonidos  en  el  espacio  virtual.  Se  requieren,  como  mínimo,  4  parlantes, 
preferentemente iguales, ubicados en los vértices de un cuadrado (o rectángulo) tan 
grande como se necesite. Los parlantes apuntan al centro, formando una ubicación 
ideal en la que los oyentes sentirán estar rodeados por un espacio sonoro en el plano 
horizontal. Claramente, a mayor cantidad de parlantes, la precisión en la simulación 
del espacio sonoro es mayor. Si se decidiera trabajar tanto en el eje horizontal como 
vertical, la cantidad mínima de parlantes necesaria sería de 8. Nuestra implementación 
utiliza la tercer columna de los archivos de datos para variar la localización de la 
fuente virtual en el plano horizontal. 
4. Conclusiones y trabajo a futuro
    El estado actual de nuestra implementación es más bien prototípico, sin embargo 
las lecciones aprendidas son fundamentales para seguir adelante. Un factor que no fue 
tenido en cuenta en esta primer versión es el de la acústica de la sala donde se instala 
el sistema. Luego de algunas pruebas con individuos, quedó claro que la simulación 
por medio de ambisonics pierde cierta precisión en una habitación asimétrica y sin 
tratamiento acústico. 
   Además, el proyecto original no incluye ningún tipo de interactividad con el oyente. 
Los últimos artículos en sonificación hablan de la sonificación interactiva como el 
futuro  de  esta  disciplina,  por  lo  que  se  plantea  como  una  posibilidad  cierta. 
Evidentemente,  es  posible  modificar  los  parámetros  de  la  instalación  utilizando 
información expresiva del usuario, por ejemplo a través de análisis de movimientos o 
gestos  capturados  por  video.  Hemos comenzado a  explorar  estas  posibilidades  en 
colaboración con investigadores de la Unidad de Gráficos y Visión por Ordenador de 
la  Universidad  de  Islas  Baleares.  Para  esto,  hemos  presentado  un  proyecto  de 
coolaboración académica a la Agencia Española de Coperación Internacional para el 
Desarrollo.
5.
