Técnicas de selección de centros basadas en histogramas de distancia
﻿El diseño de índices en espacios métricos para el procesamiento eficiente de consultas por similitud es un tema de investigación emergente. Una amplia clase de índices en espacios métricos se construyen dividiendo el espacio en
zonas tan compactas como sea posible. Por cada zona se almacena un elemento
representativo, llamado centro, e información adicional que permiten descartar la
zona completa durante una búsqueda, sin tener que calcular la distancia entre los
elementos de la zona y el objeto de búsqueda. Si bien los centros seleccionados no
afectan la efectividad del índice, son cruciales para su eficiencia. En este artículo
presentamos nuevas políticas para la selección de centros basándonos en la información que brindan los histogramas de distancia. Mostramos que las mismas son
competitivas evaluando su desempeño sobre el índice Geometric Near-neighbor
Access Tree (GNAT).
Palabras claves: Espacios Métricos, Índices, Selección de Centros.
1. Introducción
El concepto de búsquedas por similitud involucra buscar elementos de una base de
datos que sean similares o cercanos a uno dado. Este proceso de búsqueda surge junto a
la necesidad de almacenamiento de nuevos tipos de datos tales imágenes, sonido, video.
Estructurar este tipo de datos en registros con campos completamente comparables para
adecuarlos a la tecnología tradicional de bases de datos resulta difícil y hasta imposible
en algunos casos. Aún cuando tal estructuración pudiera hacerse, las consultas que se
pueden satisfacer con la tecnología tradicional no son de interés para este tipo de datos.
En [4] se muestra que este problema se puede formalizar utilizando el modelo de
espacios métricos. Un espacio métrico es un par (U , d), donde U es un universo de objetos y d una función de distancia definida entre ellos que mide cuán diferentes son.
Esta función d cumple con las propiedades características de una función de distancia: positividad ( d(, ? ) ≥ 0), simetría (d(, ? ) = d(?, ?)) y desigualdad triangular (d( , ? ) ≤ d( , ? ) + d(?, ?) ). Una de las consultas típicas en este nuevo modelo de bases de datos es la búsqueda por rango que denotaremos con (q, r)
d
: dado
un elemento q ∈ U , al que llamaremos query y un radio de tolerancia r se recuperan los objetos de la base de datos cuya distancia a q no sea mayor que r, es decir,
(q, r)
d
= {u ∈ U : d(q, u) ≤ r}.
El tiempo total de resolución de una búsqueda contiene tres términos, a saber: T=
#evaluaciones de d× complejidad(d) + tiempo extra de CPU + tiempo de I/O . En
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1037
muchas aplicaciones la evaluación de la función d es tan costosa que las demás componentes de la fórmula anterior pueden ser despreciadas. Éste es el modelo usado en este
trabajo; por consiguiente, nuestra medida de complejidad será la cantidad de evaluaciones de la función de distancia d.
Una forma trivial pero a la vez ineficiente de resolver una búsqueda por rango es
examinando exhaustivamente la base de datos comparando la query con todos los elementos de la base de datos. Para evitar esta situación, se preprocesa la base de datos
por medio de un algoritmo de indexación con el objetivo de construir una estructura de
datos o índice, diseñada para ahorrar cálculos en el momento de resolver una búsqueda.
En [4] se presenta un desarrollo unificador de las soluciones existentes en la temática.
Allí se muestra que se pueden distinguir dos grupos de algoritmos de indexación: basados en pivotes y basados en particiones compactas.
En este trabajo estamos interesados en los algoritmos basados en particiones compactas. Estos algoritmos basan la construcción del índice en un grupo de elementos preseleccionados denominados centros. Los centros seleccionados durante la construcción
del índice no afectan la efectividad del mismo pero son cruciales para su eficiencia.
En [3] se proponen dos técnicas de selección de centros basadas en los conceptos
de núcleo duro y núcleo blando [1]. Una de ellas, denominada high density zone, selecciona los centros desde el conjunto de elementos pertenecientes al núcleo duro. Los
autores muestran que esta técnica logra importantes reducciones en la cantidad de evaluaciones de distancias cuando se la compara con una selección aleatoria de centros.
Sin embargo esta técnica no usa el núcleo duro del espacio sino que usa lo que un sólo
elemento (el centro inmediatamente anterior) identifica como núcleo duro.
En este trabajo hemos abordado el diseño de políticas de selección de centros basándonos en la información que brindan los histogramas de distancia. La idea es que
la selección se realice desde el núcleo duro real del espacio y no desde la visión que
un sólo elemento tiene de él. Para probar empíricamente el desempeño de las técnicas
diseñadas, se seleccionó el índice Geometric Near-neighbor Access Tree (GNAT).
Este artículo está organizado de la siguiente manera: en la sección 2 describimos el
trabajo relacionado dando un breve explicación del GNAT y de las políticas de selección
de centros existentes basadas en histogramas locales. En la sección 3 presentamos nuestro aporte, describiendo las nuevas políticas de selección de centros diseñadas. En la
sección 4 realizamos la evaluación experimental de las mismas y finalizamos en la
sección 5 presentando las conclusiones y los trabajos a desarrollar en el futuro.
2. Trabajo Relacionado
2.1. Indexación en Espacios Métricos
En [4] se muestra que todos los enfoques para la construcción de índices en espacios métricos consisten en particionar el espacio en clases de equivalencia e indexar las
clases de equivalencia. Luego, durante la búsqueda, por medio del índice se descartan
algunas clases, y se busca exhaustivamente en las restantes. La diferencia entre los distintos algoritmos radica en cómo construyen esta relación de equivalencia. Básicamente
se pueden distinguir dos grupos: algoritmos basados en pivotes y algoritmos basados
en particiones compactas.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1038
u10
u13u5
u4
u2
u12
u3
u7
u1
u15
u14
u8
u9
u2   u5   u3   u9
u10 u12 u6 u14 u13 u8u4u7 u11 u15 u1
u11 u6
Figura 1. División generada por los centros u
2
, u
3
, u
5
, u
9
(derecha). Primer nivel del árbol
(izquierda).
En este trabajo estamos interesado en los algoritmos basados en particiones compactas, específicamente hemos seleccionado para nuestros experimentos el Geometric
Near-neighbor Access Tree (GNAT) [2]. Este índice define la relación de equivalencia
teniendo en cuenta la cercanía de los elementos a un conjunto preseleccionado de objetos denominados centros: dos elementos son equivalentes si tienen al mismo centro c
como su centro más cercano.
La construcción de un GNAT de aridad m procede de la siguiente manera: en el
primer nivel se seleccionan m centros c
1
, c
2
, · · · , c
m
de U , que se almacenan en el
nodo raíz. A cada centro c
i
se le asocia el conjunto U
?
?
formado por aquellos objetos que
están más cerca de c
i
que de cualquier otro centro c
j
. Para cada U
?
?
, si su cardinalidad
es mayor que m se construye recursivamente un GNAT, caso contrario se construye un
nodo terminal con los elementos en U
?
?
. En la figura 1 se muestra un ejemplo de la
construcción del primer nivel del GNAT considerando los centros u
2
, u
3
, u
5
y u
9
.
En cada nodo del GNAT se almacena además una tabla, a la que denotaremos
con ρ, de tamaño O(m2). Esta tabla mantiene información sobre las distancias mínimas y máximas, desde el centro c
i
a los conjuntos U
?
j
: ρ
i,j
= [mn
x∈U
c
j
d(c
i
, ?),
m?
x∈U
c
j
d(c
i
, ?)] con , ? = 1, . . . ,m. Esta información, junto con la desigualdad
triangular, se usa en el momento de la búsqueda para descartar subárboles. Para una
búsqueda por rango (q, r)
d
, se compara q con algún centro c
i
, y se descartan todos aquellos centros c
j
( y sus correspondientes U
?
j
) tales que: [d(q, c
i
)−r, d(q, c
i
)+r]∩ρ
i,j
= ∅
Este proceso se repite hasta que ningún centro pueda descartarse. La búsqueda continúa luego recursivamente en aquellos subárboles no eliminados. Durante este proceso
se agregan al resultado todos aquellos centros c
i
tales que d(c
i
, q) ≤ r.
2.2. Políticas de selección de centros basadas en histogramas locales
Uno de los principales obstáculos en el diseño de buenas técnicas de indexación es
lo que se conoce con el nombre de maldición de la dimensionalidad. El concepto de
dimensionalidad está relacionado con el nivel de dificultad al buscar en un determinado
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1039
2r
d(c,q) d(c,x)
2r
d(c,q) d(c,x)
Figura 2. Histogramas de baja dimensionalidad (derecha), y de alta dimensionalidad (izquierda)
espacio métrico. La dimensión de un espacio métrico se define en [4] como ρ = µ
2
2σ
2
,
siendo µ y σ2 la media y la varianza respectivamente de su histograma de distancias. Es
decir que, a medida que la dimensionalidad intrínseca crece, la media crece y su varianza se reduce. Esto significa que el histograma de distancia se concentra más alrededor
de su media, lo que influye negativamente en los algoritmos de indexación.
La figura 2 da una idea intuitiva de por qué el problema de búsqueda se torna más
difícil cuando el histograma es más concentrado. Los histogramas de la figura representan posibles distribuciones de distancias respecto de algún elemento c (histogramas
locales respecto de c). Considerando una búsqueda (q, r)
d
, las áreas sombreadas de la
figura muestran los puntos que no podrán descartarse si se utiliza c como centro. Puede
observarse que a medida que el histograma se concentra más alrededor de su media,
disminuye la cantidad de puntos que pueden descartarse usando como dato d(c, q). Este
fenómeno es independiente de la naturaleza del espacio métrico, y nos brinda una forma
de cuantificar cuán difícil es una búsqueda sobre el mismo.
El proceso de descarte de las búsquedas por similitud en espacios métricos es sensible a la distribución de los elementos en el espacio. Es decir, en las zonas del espacio con
mayor concentración de elementos, el proceso de descarte de zonas durante la búsqueda
se hace más dificultoso que en otras áreas de menor concentración de elementos. En [1]
los autores hacen uso de histogramas de distancias para definir el concepto de núcleo
duro y núcleo blando de un espacio métrico. El núcleo duro está formado por aquellos elementos que se ubican en la zona de mayor concentración de elementos, que se
corresponde con la zona que se encuentra alrededor de la media del histograma de distancias, si el mismo tiene forma de campana de Gauss; el núcleo blando está formado
por el resto de los elementos en el espacio métrico.
En [3] se proponen dos técnicas de selección de centros basadas en los conceptos
de núcleo duro y núcleo blando. Una de ellas, denominada closer element consiste en
seleccionar los centros desde el conjunto de elementos pertenecientes al núcleo blando
y la otra, denominada high density zone, consiste en elegirlos desde el conjunto de
elementos pertenecientes al núcleo duro. Los autores muestran que high density zone es
la más competitiva. En esta técnica se selecciona el primer centro c
1
de manera random.
Una vez seleccionado el centro c
i
, el centro c
i+1
se elige desde la zona que c
i
considera
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1040
su núcleo duro. Para esto, se calcula el histograma local de c
i
y se selecciona c
i+1
desde
el conjunto de elementos pertenecientes a la zona central de dicho histograma local.
Los autores muestran que high density zone es la más competitiva logrando importantes
reducciones en la cantidad de evaluaciones de distancias cuando se la compara con una
selección aleatoria de centros.
Notar que estas técnicas no utilizan los núcleos duro y blando del espacio métrico,
sino que en cada paso crean el histograma local del centro elegido en el paso anterior y
seleccionan el próximo centro basándose solamente en lo que el centro anterior considera como núcleo duro o núcleo blando (que puede no corresponderse con los núcleos
reales del espacio).
3. Nuevas Políticas de Selección de Centros
La política high density zone (HD) explicada en la sección anterior, basa la selección
del centro c
i+1
en el histograma local del centro c
i
. Si el centro c
i
pertenece a una zona
donde la distribución de los elementos es similar a la distribución de los elementos del
espacio, el histograma local se aproxima al histograma del espacio. Caso contrario, su
histograma difiere significativamente del histograma del espacio métrico, lo que implica
que existe una alta probabilidad de que el próximo centro elegido no pertenezca al
núcleo duro del espacio.
Una observación importante a tener en cuenta aquí es que el histograma local puede
ser muy diferente del histograma global pero si los histogramas locales de diferentes
puntos de referencia son similares, entonces podemos predecir a través de ellos la distribución de los elementos del espacio.
La política que hemos diseñado, a la que llamaremos global high density (GHD) intenta subsanar la debilidad de HD, aproximando el histograma del espacio métrico por
medio de la intersección de histogramas locales de varios centros y no sólo el inmediato anterior. La política procede de la siguiente manera: el primer centro c
1
es elegido
aleatoriamente desde el conjunto de elementos en el espacio. El segundo centro c
2
se
selecciona desde el conjunto de elementos que se ubican en la zona que c
1
identifica
como su núcleo duro. El centro c
3
se elige desde el conjunto formado por la intersección del lo que c
1
y c
2
identifican como sus núcleos duros. En general, el centro c
i
se
seleccionará desde el grupo de elementos que están presentes en la intersección de lo
que cada uno de los centros anteriores ve como núcleo duro. Si denotamos con nd
i
al
conjunto de elementos que c
i
identifica como núcleo duro, el centro c
i+1
se elige del
conjunto θ = nd
1
∩ nd
2
∩ . . . ∩ nd
i
.
En base a los lineamientos presentados en [1], los elementos que se encuentran
en la zona que c
i
identifica como su núcleo duro son aquellos cuya distancia a c
i
es
cercana a la media del histograma local de c
i
. Es decir, un elemento e pertenece a lo
que c
i
identifica como su núcleo duro si e ∈ [µ − rc, µ + rc] donde µ es la media del
histograma local de c
i
y rc es un número entero al que llamaremos radio de corte. El
valor del radio de corte se establece empíricamente.
A medida que se avanza en la construcción del índice, disminuye la cantidad de
elementos en cada subárbol y aumenta la probabilidad de que el conjunto θ sea vacío.
Cuando esto sucede se hace imposible utilizar la técnica diseñada. Para subsanar este
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1041
problema existen varias posibilidades. Una de ellas es continuar de manera aleatoria
con la selección de centros. Otra posibilidad es iniciar un nuevo ciclo de cálculo del
conjunto θ. Describimos a continuación las variaciones implementadas, junto con la
sigla con la que haremos referencia a cada una de ellas en lo que resta de este artículo.
GHD-A: Continuar con selección aleatoria de centros.
GHD-I
1
: Seleccionar el próximo centro c
i
aleatoriamente desde el conjunto de elementos contenidos en ese subárbol, e iniciar un nuevo ciclo intersecciones haciendo
θ = nd
i
.
GHD-I
2
: Seleccionar el próximo centro c
i
desde el último conjunto θ no vacío, e iniciar un nuevo ciclo intersecciones haciendo θ = nd
i
.
4. Resultados Experimentales
Los experimentos se realizaron sobre diccionarios de palabras usando como función de distancia la distancia de edición. Esta distancia es una función discreta que
mide la cantidad de caracteres que hay que agregar, cambiar y/o eliminar a una palabra
para obtener la otra. Se utilizaron los diccionarios Español, Alemán, Inglés e Italiano.
Por cuestiones de espacio sólo mostramos los resultados obtenidos con el diccionario
Español, encontrándose disponibles los restantes para quien así lo requiera.
De cada diccionario se extrajo una muestra aleatoria del 10% del total de los elementos para ser utilizada como elementos de consulta y se indexó el 0 % restante.
Se realizaron búsquedas por rango (q, r)
d
usando como radio de búsqueda los valores
1, 2, 3, 4. El espacio fue indexado utilizando el algoritmo GNAT descripto en la sección
2.1, usando para la aridad los valores 32, 64, 12? y 26 .
Como ya mencionamos, todas las variaciones de GHD utilizan un valor de radio
de corte (rc) para definir lo que cada centro identifica como su núcleo duro. Siguiendo
los lineamientos presentados en [1], el valor de rc fue establecido experimentalmente
realizando pruebas con rc = 1, 2, 3, 4 y ?.
El desarrollo de los experimentos se dividió en tres etapas. La primera de ellas
tuvo como objetivo establecer el valor más adecuado de rc para las políticas GHDA, GHD-I
1
y GHD-I
2
. La segunda etapa, consistió en comparar estas tres variaciones
entre sí para identificar la variación de mejor desempeño a fin de, en la última etapa,
compararla con la política HD.
4.1. Estableciendo el radio de corte rc
El radio de corte utilizado por las distintas variaciones tiene una importante influencia en la calidad de los centros seleccionados y en consecuencia en la performance de
la búsqueda. El objetivo de esta etapa de evaluación es determinar el valor óptimo para
el radio de corte en cada variante presentada.
Se realizaron experimentos para cada variación de GHD, con diferentes radios de
búsqueda y GNATs de diferentes aridades. La figura 3 muestra los resultados obtenidos
con las distintas variantes utilizando GNAT de aridad 64. Sobre el eje ? están representados los radios de búsqueda y sobre el eje ? el número medio de evaluaciones de
distancia realizadas para resolver la búsqueda.
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1042
 15000
 20000
 25000
 30000
 35000
 40000
 45000
 50000
 55000
 1  2  3  4
Com
par
aci
one
s
Radio Búsqueda
Aridad 64
rc1rc2rc3rc4rc5  10000
 15000
 20000
 25000
 30000
 35000
 40000
 45000
 50000
 55000
 60000
 1  2  3  4
Com
par
aci
one
s
Radio Búsqueda
Aridad 64
rc1rc2rc3rc4rc5
 15000
 20000
 25000
 30000
 35000
 40000
 45000
 50000
 55000
 1  2  3  4
Com
par
aci
one
s
Radio Búsqueda
Aridad 64
rc1rc2rc3rc4rc5
Figura 3. Resultados sobre un GNAT de aridad 4 , usando distintos valores de ??.
Como puede observarse, en algunas de las variantes propuestas, el mismo valor
de rc obtiene la menor cantidad de evaluaciones de distancia para todos los radios de
búsqueda. Este es el caso de GHD-A y GHD-I
1
en las que el radio de corte rc=4 logra
el mejor desempeño. Sin embargo, esto no ocurre con la variación GHD-I
2
, donde el
radio de corte con mejor desempeño varía según el radio de búsqueda utilizado. En
estos casos, es importante tener en cuenta que los radios de búsqueda utilizados en las
consultas no son conocidos al momento de indexar, por lo que la elección del valor de
rc no puede depender del radio de búsqueda. Esta es la razón por la que seleccionamos
como mejor valor para GHD-I
2
a rc = 4, que si bien no obtiene el mejor desempeño
para todos los casos, mantiene una buena performance en cada uno de ellos.Este mismo
criterio fue utilizado cuando se analizaron los resultados para las aridades 32, 12? y
26 . El cuadro 1 muestra los valores de rc que resultaron seleccionados en cada caso.
4.2. Evaluando las distintas variantes de GHD
Habiendo identificado el valor más adecuado para rc, nos enfocamos en identificar
la variante más adecuada para GHD.
La figura 4 muestra los resultados obtenidos, con GNAT de distintas aridades para
cada uno de los radios de búsqueda. Sobre el eje ? se representan las distintas aridades
utilizadas y sobre el eje ? la cantidad de evaluaciones de distancias que fueron requeriCACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1043
Aridad GHD-A GHD-I1 GHD-I2
2 ?? ? ? ?? ? ? ?? ? ?
4 ?? ? 4 ?? ? 4 ?? ? 4
28 ?? ? 2 ?? ? 2 ?? ? ?
2? ?? ? ? ?? ? 4 ?? ? ?
Cuadro 1. Valores de ?? para las distintas variantes de la política GHD.
das para resolver la búsqueda. Los valores presentados en esta gráfica se obtuvieron
utilizando el valor de rc ya establecido para cada una de las aridades (ver cuadro 1).
En términos generales, se puede observar que todas las variantes mantienen el mismo comportamiento para las distintas aridades, esto es: para aridades mayores a 64,
aumentar la aridad del árbol disminuye la cantidad de evaluaciones de distancia realizadas durante una búsqueda (resultado ya ampliamente conocido).
Lo que varía para las distintas variantes de GHD es el porcentaje de mejora conseguido al aumentar la aridad del árbol. La variante más beneficiada es la GHD-A,
resultando ser la más competitiva para las aridades 12? y 26 , en todos los radios de
búsqueda. Sin embargo, con aridad 32 es superada por la variación GHD-I
1
y con aridad 64 ambas obtienen los mismos resultados. La variación GHD-I
1
es la que menos
se beneficia con el aumento de aridad, lo que provoca que sea la menos competitiva
de las tres para aridades grandes. Respecto de la variante GHD-I
2
es superada siempre por H?
A
alcanzando esta última hasta un 7% de reducción en la cantidad de
evaluaciones de distancia realizadas para resolver una búsqueda.
Recordemos que la técnica GHD basa la selección del próximo centro en el conjunto
θ, que representa lo que se ha identificado como núcleo duro hasta el último centro
elegido. Las distintas variantes de esta técnica optan por diferentes alternativas en el
momento en que el conjunto θ resulta vacío. Los experimentos nos indican que iniciar
nuevos ciclo de cálculo del conjunto θ influye de manera negativa en la selección de
los centros degradando la performace del índice. Esto nos permite intuir que cuando θ
se convierte en vacío, ya no es posible lograr acumular la información suficiente que
permita seleccionar centros que mejoren la calidad de filtrado de los centros ya elegidos.
Para aridades grandes θ resulta vacío mas rápidamente, por esta razón en estas aridades
GHD-A es la técnica más conveniente. Para aridades pequeñas el conjunto θ tarda más
en vaciarse y en consecuencia GHD-I
1
puede realizar una mejor selección de centros
que GHD-A.
4.3. HD versus GHD
Habiendo establecido que las variantes más convenientes son GHD-A y GHD-I
1
,
procedimos a compararlas con HD. La figura 5 muestra la cantidad de comparaciones
requeridas con GNATs de diferentes aridades. Se muestran los resultados para radio 1
y 2 arriba y para los radios 3 y 4 abajo.
Como se puede observar ambas políticas mantienen el mismo comportamiento frente
a las distintas aridades: en aridades mayores a 64, ambas políticas mejoran su rendimiento a medida aumenta la aridad del árbol. Sin embargo, la política GHD-A logra obtener
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1044
 7000
 8000
 9000
 10000
 11000
 12000
 13000
 14000
 15000
 32  64  128  256
Com
par
aci
one
s
Aridad
Radio de búsqueda 1
GHD−AGHD−I1GHD−I2
 18000
 20000
 22000
 24000
 26000
 28000
 30000
 32  64  128  256
Com
par
aci
one
s
Aridad
Radio de búsqueda 2
GHD−AGHD−I1GHD−I2
 30000
 32000
 34000
 36000
 38000
 40000
 42000
 44000
 32  64  128  256
Com
par
aci
one
s
Aridad
Radio de búsqueda 3
GHD−AGHD−I1GHD−I2
 44000
 46000
 48000
 50000
 52000
 54000
 32  64  128  256
Com
par
aci
one
s
Aridad
Radio de búsqueda 4
GHD−AGHD−I1GHD−I2
Figura 4. Resultados de las variantes de GHD, para radios de búsqueda ?y2 (arriba), ?y4 (abajo).
un mejor desempeño con la mayoría de las aridades para todos los radios de búsquedas.
La variación GHD-A presenta una tendencia a aumentar la ventaja respecto de HD a
medida se aumenta la aridad del árbol, es decir, a medida aumenta la cantidad de centros
a seleccionar en cada nodo. Para analizar este comportamiento, es importante recordar
que mientras la política de selección HD sólo utiliza el histograma local de cada centro
para seleccionar el siguiente, la variación GHD-A se basa en la información acumulada
por todos los centros seleccionados anteriormente como una manera de aproximar el
núcleo duro del espacio. En situaciones en las que es necesario seleccionar una mayor
cantidad de centros en cada nodo, la variación GHD-A aumenta la posibilidad de acumular más información en el conjunto θ y así aproximar de mejor manera el núcleo
del espacio. Con GNAT de aridad 32 las políticas presentan un rendimiento similar en
todos los radios de búsqueda, obteniendo una pequeña ventaja GHD-I
1
.
5. Conclusiones y Trabajo Futuro
En este trabajo hemos presentado una nueva política de selección de centros denominada global high density. El diseño de esta política se centró en subsanar una
debilidad identificada en la política high density zone presentada en [3]. A partir de
la nueva política global high density surgieron 3 nuevas variantes a las que denominamos GHD-A, GHD-I
1
y GHD-I
2
. Las variantes GHD-A y GHD-I
1
resultaron ser las
CACIC 2011 - XVII CONGRESO  ARGENTINO DE CIENCIAS DE LA COMPUTACIÓN 1045
 8000
 9000
 10000
 11000
 12000
 13000
 14000
 32  64  128  256
Com
par
aci
one
s
Aridad
Radio de búsqueda 1
GHD−AGHD−I1HD
 18000
 20000
 22000
 24000
 26000
 28000
 30000
 32  64  128  256
Com
par
aci
one
s
Aridad
Radio de búsqueda 2
GHD−AGHD−I1HD
 30000
 32000
 34000
 36000
 38000
 40000
 42000
 44000
 32  64  128  256
Com
par
aci
one
s
Aridad
Radio de búsqueda 3
GHD−AGHD−I1HD
 44000
 46000
 48000
 50000
 52000
 54000
 32  64  128  256
Com
par
aci
one
s
Aridad
Radio de búsqueda 4
GHD−AGHD−I1HD
Figura 5. Resultados de las comparación de GHD con la política HD.
más competitivas respecto de las restantes técnicas de selección de centros, logrando un
mejor desempeño en todas las aridades evaluadas. Esto nos indica que para seleccionar
los centros, siempre resulta más conveniente usar un aproximación del histograma real
del espacio que usar la visión que un sólo elemento tiene de él.
Respecto del trabajo futuro, nos proponemos estudiar el comportamiento de estas
variaciones con otros espacios métricos que no tengan histogramas de distancia en forma de campana como la identificada en los diccionarios de palabras.
