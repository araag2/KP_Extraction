Optimización de técnicas de compresión
﻿Optimización de Técnicas de Compresión. Paralelización, 
Análisis de la Pérdida. 
Objetivos 
Lic. Hugo D. Ramón I 
M'i. Claudia C. RussoJ 
Ing. Armando De Giust¡3 
Lahoratorio de Desarrollo e Investigación en h?formática - LIDI' 
h?fórmática - UNLP 
Es notoria la importancia creciente de la compreslon de datos en la transmisión de 
información en redes. Aspectos como seguridad, reducción de tráfico, optimización del uso del 
canal, etc., justifican las tareas de investigación y desarrollo en este tema, así como la evolución de 
los recursos tecnológicos disponibles [Russo 1995]. 
En particular, por el volumen de información en juego, en los últimos años hay un esfuerzo 
muy notorio en la optimización de algoritmos para compresión de datos utilizando diferentes 
tecnologías en software y en hardware que implican paralelización de métodos aplicando por 
ejemplo sockets y PVM. 
Como se dijo anteriomlente la compresión de datos en particular de una imagen digital 
puede facilitar su transmisión, almacenamiento y procesamiento. Una compresión considerable (un 
radio de 5 a 10 veces) se puede lograr sólo mediante algoritmos con pérdida, los que no permiten la 
recuperación exacta de la imagen original. 
Es importante el estudio de métodos de compresión adaptivos de manera de optimizar el 
radio de compresión. 
La pérdida de información al aplicar un método hace que el algoritmo seleccionado para 
comprimir así como otros algoritmos de procesamiento de imágenes no sean de aplicación universal 
(por ejemplo, imágenes médicas críticas) debido a la potencial pérdida de calidad y problemas 
consecuentes relacionados con la confiabilidad. 
Algunas de las preguntas que se deben hacer son: 
¿Cómo se decide el método de compresión? 
¿Cómo se puede optimizar el algoritmo utilizado para comprimir? 
¿Se puede optimizar el tiempo de compresión utilizando paralelismo? 
'Jefe de Trabajos Prácticos, Dedicación Exclusiva. Facultad de Ciencias Exactas, Departamento de 
Informática, U.N.L.P. E-mail: hramon@lidi.info.unlp.edu.ar. 

Profesor Adjunto, Dedicación Exclusiva, Facultad de Ciencias Exactas. Departamento de Informática, 
U.N.L.P. E-mail: crusso@lidi.info.unlp.edu.ar. 

 Investigador Principal del CONICET, Profesor Titular. Dedicación Exclusiva, Facultad de Ciencias Exactas, 
Departamento de Informática, U.N.L.P. E-mail: adegiusti@lidi.info.unlp.edu.ar. 

 Laboratorio de Investigación y Desarrollo en Infomática. Dpto. de Infomática. Fac. de Ciencias Exactas. 
Calle 50 y 115. La Plata (1900). Ss. As. Te1-fax (54) 221 422 77 07 
¿Cómo se decide si una imagcn procesada digitalmente es lo suficientemente buena para un:! 
aplicación específica. como ser, el diagnóstico por imág.enes, el almacenamiento o su uso 
educacional. etc.? 
Se tratarú de contestar estas preg.untas. 
En síntesis el aporte de esta línea de investigación es el análisis experimental de la 
implcmentación de métodos adoptivos de compresión y la viabilidad de su paralelización. teniendo 
en cuenta la relación entre calidad, tiempo y radio de compresión en imágenes digitalizadas. 
Introducción 
La compresión de una imagen digital puede facilitar su procesamiento, almacenamiento y 
transmisión. A medida que las grandes Organizaciones se vuelven cada vez más digitales y 
distribuidos. la cantidad de datos multimediales (en particular imágenes) de imágenes que tienen. 
obligan a considerar su compresión para su almacenamiento y transmisión eficiente. 
El objetivo general de la compresión es representar una imagen con la menor cantidad 
posible de bits, acelerando así la transmisión y minimizando los requerimientos de almacenamiento. 
De manera alternativa, -el objetivo es lograr la mejor fidelidad posible para una capacidad dispo:lible 
de comunicación o almacenamiento. 
En el campo de las comunicaciones visuales se ha ido mucho trabajo en la codificación de 
imágenes digitales dirigida a reducir los requerimientos de la tasa de bits (bit rate) para la 
transmisión de las imágenes. La experiencia ha mostrado que cada esquema de codificación está 
suieto a su propio y único conjunto de causas de pérdida que a menudo son difíciles de caracterizar. 
Esto se debe a la forma en la que se diseñan los esquemas de codificación para variar selectivamente 
la precisión de la representación: el observador puede ser insensible a los errores en algunas partes 
de la imagen pero no en otras. Una compresión considerable se puede lograr sólo mediante 
algoritmos con pérdida. los que no permiten la recuperación exacta de la imagen original. Esta 
pérdida de información hace que la compresión y otros algoritmos de procesamiento de imágenes 
sean controversiales debido a la potencial pérdida de calidad y problemas consecuente!' en cuanto a 
confiabilidad. Sin embargo, el uso de la tecnología debe considerarse. ya que la altem?tiva es la 
demora. daño y pérdida en la comunicación y recuperación de las imágenes. ¿Cómo se decide si una 
imagen es lo suficientemente buena como para una aplicación específica. como ser el diagnóstico. o 
el archivo o su uso educacional? 
No solo nos preocupa el almacenamiento. el mantenimiento de los datos y la calidad de los 
datos. sino también la disponibilidad en forma segura y rápida. El problema no acaba con optimizar 
el manejo local de los datos, debe resolverse también el problema de transmitirlos [Russo 1990]. 
Dentro de una red, el recurso compartido es el canal. de ahí. que la performance de la red 
dependa en gran parte del ancho de banda del canal. Si fuese posible disminuir la cantidad de 
información a transmitir, se lograría mayor fluidez en el tránsito del canal y por ende se mejoraría la 
performance global de la red. 
Es por eso que debemos poner énfasis en analizar la conveniencia de minimizar el tráfico de 
datos por la red. problema especialmente importante cuando el medio de comunicación es costoso 
(en tiempo y/o dinero). 
Básicamente se utilizan distintos métodos de compresión de información para [Russo 1995]: 
• Reducción de tiempos en la transmisión de datos 
• Menor requerimiento de memoria para almacenamiento ó 
En la actualidad se han desarrollado distintos métodos de compresión con las siguientes 
propiedades: 
• Tipo de método: con pérdida o sin pérdida de información 
• Nivel de complejidad desde el punto de vista computacional 
• Rango de datos sobre el cual el método puede ser utilizado obteniéndose resultados 
satisfactorios. 
También se pueden definir modelos de compreslon que maxlmlzen el radio utilizando 
técnicas adaptivas, explotando las características estáticas propias de cada imagen. Por ejemplo, se 
realiza la implementación del JPEG, modificando el particionamiento fijo por un particionamiento 
quadtree. 
Hay un requerimiento de mediciones precisas de las perdidas subjetivas que puedan usarse 
para predecir la calidad de una imagen. El objetivo en esta investigación es determinar estas 
mediciones de la distorsión y probar cuán bien pueden predecir la calidad de una imagen evaluada 
para un conjunto de imágenes de prueba que contienen varios deterioramientos de codificación. 
Se describen enfoques para medir la calidad de las imágenes, como, razón señal-ruido 
(SNR), evaluación subjetiva, mediciones del error crudo, las mediciones del error filtrado en donde 
las propiedades de filtrado de la visión se tienen en cuenta y las mediciones de error enmascarado en 
donde también se incluyen los procesos de enmascaramiento [Cosman 1994] [Rus so 1998]. 
La necesidad de este tipo de mediciones está particularmente reconocida en el área de la 
codificación de imágenes digitales. Mediciones de la distorsión subjetivamente relevantes que 
reflejan las evaluaciones de la calidad de una imagen que hace un espectador, harían 
considerablemente más fácil la tarea de diseñar y optimizar los esquemas de codificación. 
Las algunas de las mediciones de distorsión se basan en un modelo espacial-temporal de 
visión umbral que incorpora los procesos de filtrado y enmascaramiento [Algazi 1992]. El filtrado 
visual se lleva a cabo por caminos paralelos de estimulación e inhibición, cada uno de los cuales es 
lineal por separado pero se combinan de manera no lineal para tener en cuenta la adaptación con 
luminancia de fondo. El enmascaramiento se da en la forma de un pesado punto a punto del error 
filtrado basado en la cantidad de actividad espacial y temporal en el entorno inmediato. El error 
procesado promediado en toda la imagen se usa luego como una predicción de la calidad de la 
imagen. 
Se comparan y contrastan estas distorsiones con un conjunto de imágenes representativas de 
varios dominios de aplicación y se examinan cuán buenas son las mediciones de distorsiones que se 
pueden obtener fácilmente como la SNR para predecir las evaluaciones subjetiva, que son más caras 
en tiempo. Los ejemplos son de imágenes tradicionales que se utilizan en el área de procesamiento y 
compresión de imágenes, comprimidas utilizando JPEG estándar [Wallace 1991] [Russo 1998]. 
Por otro lado el ojo humano es menos sensible a las frecuencias espaciales altas, o bordes de 
una imagen, que a las frecuencias bajas, o texturas de una imagen; por ejemplo, si en un monitor 
desplegamos un patrón continuo de pixeles blancos y negros alternados uno y otro, el ojo humano 
tiende a detectar este patrón como un gris uniforme y continuo en lugar del "mosaico" que tiene a la 
vista. Esta deficiencia se explota codificando con pocos bits los coeficientes que representan 
frecuencias altas y con muchos bits los de frecuencias bajas [Shaddock, 1992]. 
El sistema visual humano es lo suficientemente complejo como para que mucho de nuestro 
entendimiento provenga de tratar de entender fenómenos que se revelan por medio de experimentos 
físicos. 
Dado que los ojos actúan como filtro, es bueno conocer algunas de las características 
visuales a las que puede ser sensible, como ejemplo Sensibilidad al Contraste, Ruido, Bandas de 
Mach, Co.ntraste de Luminosidad. 
esuItados Obtenidos y Líneas actuales de lID 
Se han realizado experiencias de comparación de implementaciones paralelas y lineales de 
algoritmos de compresión de datos que utilizan diccionario, utilizando una red de co.mputado.ras 
heterogéneas para la distribución y comunicación de procesos aplicando sockets y PVM. 
Se ha analizado e implementado el algoritmo de Huffman estático y dinámico con el objeto 
de comparar las implementaciones paralelas. Se ha analizado la eficiencia de la solución par 
diferentes arquitecturas de hardware y software. 
Se experimentaron implementaciones algorítmicas bajo UNIX dado que se poseían 
herramientas de desarrollo standard como memoria compartida, semáfo.ros y sockets, pensando. luego 
en migrar hacia o.tro tipo. de software y hardware (DSP's, Transputers, etc.) [Tinetti 1995]. 
Se desarrolló un algoritmo JPEG con particionamiento adaptivo y se realizaron 
comparaciones con el método JPEG estándar y otras técnicas de compresión con perdida. 
Actualmente se está trabajando en un modelo de experimentación para la evaluación 
subjetiva de la pérdida, comparando los resultados con medidas objetivas clásicas (relación 
señal/ruido por ejemplo). Se ha diseñado un experimento, que incluye tres clases de imágenes 
(clasificadas por su histograma) y dos clases de observadores. Además se desarrollo un software 
interactivo para la calificación de las diferentes imágenes para luego poder realizar las valoraciones 
correspondientes. 
También se está investigando en la compreslon de imágenes de video con animación, 
orientado al tratamiento paralelo para la transmisión en tiempo real. 
